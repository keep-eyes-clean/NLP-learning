{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1: Classifying Names with a Character-Level RNN\n",
    "\n",
    "A character-level RNN reads words as a series of characters - outputting a prediction and “hidden state” at each step, feeding its previous hidden state into each next step. We take the final prediction to be the output, i.e. which class the word belongs to.\n",
    "\n",
    "Specifically, we’ll train on a few thousand surnames from 18 languages of origin, and predict which language a name is from based on the spelling.\n",
    "\n",
    "## pytorch 深度学习框架\n",
    "\n",
    "打算手撕这个框架，进而加深对于深度学习的实现的理解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Torch\n",
    "\n",
    "use GPU acceleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "print(torch.get_default_device())\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data\n",
    "\n",
    "从官网上下载了数据\n",
    "- 1. 定义和清理我们的数据。\n",
    "    - 最初，我们需要将 Unicode 转换为普通的 ASCII，以限制 RNN 输入层的复杂性。\n",
    "    - 这是通过将 Unicode 字符串转换为 ASCII，并仅允许一小部分指定的字符来实现的。\n",
    "\n",
    "- 2. 将unicode字符转换为ascii字符的方式\n",
    "    - 归一化处理：使用unicodedata.normalize('NFD', s)将输入字符串s转换为其分解形式\n",
    "    - 过滤掉组合符号：通过列表推导式遍历归一化后的每个字符c，并检查其类别是否不是'Mn'（Mark, Nonspacing），即非间距标记\n",
    "    - 字符集限制：进一步筛选出那些存在于预定义集合allowed_characters中的字符\n",
    "    - 拼接结果：最后，使用''.join(...)将所有符合条件的字符连接起来形成一个新的字符串，并返回这个字符串。\n",
    "    - 貌似中文不行，但是其他字符可以"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 你好Ślusàrski\n",
      "Converted to ASCII: Slusarski\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import unicodedata\n",
    "\n",
    "# 生成ascii字符：'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ.,;'\n",
    "allowed_characters = string.ascii_letters + \".,;\"\n",
    "n_letters = len(allowed_characters)\n",
    "\n",
    "# 将unicode字符转换为ascii字符，详情见于“https://stackoverflow.com/a/518232/2809427”\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD',s) if unicodedata.category(c) != 'Mn' and c in allowed_characters)\n",
    "\n",
    "# Example usage\n",
    "original_string = \"你好Ślusàrski\"\n",
    "ascii_string = unicodeToAscii(original_string)\n",
    "print(f\"Original: {original_string}\")\n",
    "print(f\"Converted to ASCII: {ascii_string}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 将字符串变成tensor格式\n",
    "采用 *one-hot* 编码\n",
    "\n",
    "- lineToTensor(line) 解释：\n",
    "    - 初始化张量：使用torch.zeros(len(line), 1, n_letters)创建一个全零的三维张量。张量的形状是(len(line), 1, n_letters)，其中：\n",
    "        - len(line) 是字符串中字符的数量。\n",
    "        - 1 表示批次大小（batch size），这里假设每次处理一个字符。\n",
    "        - n_letters 是字母表中不同字符的数量。\n",
    "    - 遍历字符串中的每个字符：使用enumerate(line)来获取字符串中每个字符及其对应的索引li和字符本身letter。\n",
    "    - 设置独热向量：对于每个字符，找到其在字母表中的索引（通过letterToIndex(letter)函数），然后将对应位置的值设置为1。这样就形成了一个独热向量，表示该字符。\n",
    "    - 返回张量：最终返回构造好的三维张量。\n",
    "\n",
    "对于其他RNN任务，我也可以使用类似的方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The letter 'a' becomes tensor([[[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0.]]])\n",
      "The name 'Ahn' becomes tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "def letterToIndex(letter):\n",
    "    return allowed_characters.find(letter)\n",
    "\n",
    "def lineToTensor(line):\n",
    "    tensor = torch.zeros(len(line), 1, n_letters)\n",
    "    for li, letter in enumerate(line):\n",
    "        tensor[li][0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "print (f\"The letter 'a' becomes {lineToTensor('a')}\")\n",
    "print (f\"The name 'Ahn' becomes {lineToTensor('Ahn')}\") #notice 'A' sets the 27th index to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and DataLoader\n",
    "\n",
    "制作自己的数据集：\n",
    "- NameDataset(Dataset):\n",
    "    - 定义了一个名为NamesDataset的类，并使其继承自torch.utils.data.Dataset\n",
    "\n",
    "    - 设置属性：\n",
    "        - self.data_dir: 存储数据目录路径\n",
    "        - self.load_time: 记录数据加载的时间\n",
    "        - labels_set: 用于存储唯一的标签集合\n",
    "\n",
    "    - 初始化列表：\n",
    "        - self.data:存储所有姓名\n",
    "        - self.data_tensor:存储每个姓名对应的张量表示\n",
    "        - self.labels:存储每个姓名对应的标签\n",
    "        - self.labels_tensors:存储每个标签对应的张量表示\n",
    "\n",
    "    - 查找文件内容：\n",
    "        - 使用glob.glob查找指定目录下所有的.txt文件。\n",
    "        - 对于每个文件名，提取其基础名称作为标签。\n",
    "    \n",
    "    - 读取文件内容：\n",
    "        - 打开每个文件并读取内容，按行分割得到姓名列表。\n",
    "        - 将每个姓名及其对应的标签添加到相应的列表中。\n",
    "        - 同时，使用lineToTensor函数将每个姓名转换为张量，并添加到self.data_tensors中。\n",
    "    \n",
    "    - 唯一标签列表：\n",
    "        - 将labels_set转换为列表self.labels_uniq，以便后续查找索引。\n",
    "    \n",
    "    - 标签tensor：\n",
    "        - 遍历所有标签，将其转换为对应的整数索引，并创建一个长整型张量。\n",
    "        - 将这些张量添加到self.labels_tensors中。\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 20074\n",
      "Label tensor: tensor([2])\n",
      "Data tensor: tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0.]]])\n",
      "Data label: Russian\n",
      "Data item: Abaev\n"
     ]
    }
   ],
   "source": [
    "from io import open\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class NamesDataset(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "        self.load_time = time.localtime()\n",
    "        labels_set = set()\n",
    "\n",
    "        self.data = []\n",
    "        self.data_tensors = []\n",
    "        self.labels = []\n",
    "        self.labels_tensors = []\n",
    "\n",
    "        text_files = glob.glob(os.path.join(data_dir, \"*.txt\"))\n",
    "        for filename in text_files:\n",
    "            label = os.path.splitext(os.path.basename(filename))[0]\n",
    "            labels_set.add(label)\n",
    "            lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "            for name in lines:\n",
    "                self.data.append(name)\n",
    "                self.data_tensors.append(lineToTensor(name))\n",
    "                self.labels.append(label)\n",
    "\n",
    "        self.labels_uniq = list(labels_set)\n",
    "        for idx in range(len(self.labels)):\n",
    "            temp_tensor = torch.tensor([self.labels_uniq.index(self.labels[idx])], dtype=torch.long)\n",
    "            self.labels_tensors.append(temp_tensor)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        data_item = self.data[idx]\n",
    "        data_label = self.labels[idx]\n",
    "        data_tensor = self.data_tensors[idx]\n",
    "        label_tensor = self.labels_tensors[idx]\n",
    "\n",
    "        return label_tensor, data_tensor, data_label, data_item\n",
    "\n",
    "                \n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Assuming there are files like 'English.txt', 'Spanish.txt' in the directory './data/names'\n",
    "    dataset = NamesDataset('/harddisk1/SZC-Project/NLP-learning/RNNs/data/names')\n",
    "    print(f\"Number of samples: {len(dataset)}\")\n",
    "    label_tensor, data_tensor, data_label, data_item = dataset[1]\n",
    "    print(f\"Label tensor: {label_tensor}\")\n",
    "    print(f\"Data tensor: {data_tensor}\")\n",
    "    print(f\"Data label: {data_label}\")\n",
    "    print(f\"Data item: {data_item}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/harddisk1/SZC-Project/NLP-learning/RNNs/data/names/Russian.txt',\n",
       " '/harddisk1/SZC-Project/NLP-learning/RNNs/data/names/Arabic.txt',\n",
       " '/harddisk1/SZC-Project/NLP-learning/RNNs/data/names/German.txt',\n",
       " '/harddisk1/SZC-Project/NLP-learning/RNNs/data/names/Portuguese.txt',\n",
       " '/harddisk1/SZC-Project/NLP-learning/RNNs/data/names/Scottish.txt',\n",
       " '/harddisk1/SZC-Project/NLP-learning/RNNs/data/names/Spanish.txt',\n",
       " '/harddisk1/SZC-Project/NLP-learning/RNNs/data/names/Korean.txt',\n",
       " '/harddisk1/SZC-Project/NLP-learning/RNNs/data/names/Italian.txt',\n",
       " '/harddisk1/SZC-Project/NLP-learning/RNNs/data/names/Greek.txt',\n",
       " '/harddisk1/SZC-Project/NLP-learning/RNNs/data/names/Czech.txt',\n",
       " '/harddisk1/SZC-Project/NLP-learning/RNNs/data/names/Vietnamese.txt',\n",
       " '/harddisk1/SZC-Project/NLP-learning/RNNs/data/names/Japanese.txt',\n",
       " '/harddisk1/SZC-Project/NLP-learning/RNNs/data/names/French.txt',\n",
       " '/harddisk1/SZC-Project/NLP-learning/RNNs/data/names/Chinese.txt',\n",
       " '/harddisk1/SZC-Project/NLP-learning/RNNs/data/names/Polish.txt',\n",
       " '/harddisk1/SZC-Project/NLP-learning/RNNs/data/names/English.txt',\n",
       " '/harddisk1/SZC-Project/NLP-learning/RNNs/data/names/Dutch.txt',\n",
       " '/harddisk1/SZC-Project/NLP-learning/RNNs/data/names/Irish.txt']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = '/harddisk1/SZC-Project/NLP-learning/RNNs/data/names'\n",
    "text_files = glob.glob(os.path.join(data_dir, \"*.txt\"))\n",
    "text_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用torch.utils.data.random_split对于创建的NameDataset实例进行数据集划分，划分比例：80/20（train/test）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train examples = 16060, validation examples = 4014\n"
     ]
    }
   ],
   "source": [
    "data_dir = '/harddisk1/SZC-Project/NLP-learning/RNNs/data/names'\n",
    "alldata = NamesDataset(data_dir)\n",
    "\n",
    "train_set, test_set = torch.utils.data.random_split(alldata, [0.8, 0.2])\n",
    "generator = torch.Generator(device=device).manual_seed(2025)\n",
    "\n",
    "print(f\"train examples = {len(train_set)}, validation examples = {len(test_set)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create RNN model\n",
    "\n",
    "使用 `nn.RNN` 作为模型，它支持cuDNN-accelerated kernels\n",
    "\n",
    "### 模型结构\n",
    "\n",
    "- nn.RNN 层\n",
    "- nn.Linear 层\n",
    "- nn.LogSoftmax 层\n",
    "\n",
    "### 参数\n",
    "模型参数如下：\n",
    "- input_size: 57\n",
    "- hidden_size: 128\n",
    "- output_size: 18\n",
    "\n",
    "### 数学公式证明\n",
    "\n",
    "### 图片\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.9036, -2.7271, -2.7849, -2.9307, -2.9736, -2.9782, -2.8790, -2.9417,\n",
      "         -2.8749, -2.8797, -2.8961, -2.8433, -2.9039, -2.8652, -2.9012, -2.9483,\n",
      "         -2.8349, -3.0003]], grad_fn=<LogSoftmaxBackward0>)\n",
      "('Greek', 1)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
    "        super(CharRNN, self).__init__()\n",
    "\n",
    "        self.rnn = nn.RNN(input_size, hidden_size)\n",
    "        self.h2o = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self, line_tensor):\n",
    "        rnn_out, hidden = self.rnn(line_tensor)\n",
    "        output = self.h2o(hidden[0])\n",
    "        output = self.softmax(output)\n",
    "        return output\n",
    "\n",
    "def label_from_output(output, output_labels):\n",
    "    # top_n: 包含最大值（即模型预测的最高置信度得分）\n",
    "    # top_i: 包含最大值对应的索引\n",
    "    top_n, top_i = output.topk(1) # 从张量中找到前 k 个最大值及其对应的索引\n",
    "    label_i = top_i[0].item() \n",
    "    return output_labels[label_i],label_i\n",
    "\n",
    "n_hidden = 128\n",
    "rnn = CharRNN(n_letters, n_hidden, len(alldata.labels_uniq))\n",
    "\n",
    "input = lineToTensor(\"Albert\")\n",
    "output = rnn(input)\n",
    "print(output)\n",
    "print(label_from_output(output, alldata.labels_uniq))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-2.7271]], grad_fn=<TopkBackward0>), tensor([[1]]))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_n, top_i = output.topk(1)\n",
    "top_n, top_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['English',\n",
       " 'Greek',\n",
       " 'Russian',\n",
       " 'Italian',\n",
       " 'Scottish',\n",
       " 'German',\n",
       " 'Japanese',\n",
       " 'Irish',\n",
       " 'Portuguese',\n",
       " 'Polish',\n",
       " 'Korean',\n",
       " 'Dutch',\n",
       " 'Arabic',\n",
       " 'French',\n",
       " 'Chinese',\n",
       " 'Vietnamese',\n",
       " 'Spanish',\n",
       " 'Czech']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alldata.labels_uniq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练模型\n",
    "### 步骤\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def train(rnn, training_data, n_epoch = 10, n_batch_size=64, report_every=50, learning_rate=0.1, criterion=nn.NLLLoss()):\n",
    "    \"\"\"\n",
    "    Learn on a batch of training_data for a specified number of iterations and reporting thresholds\n",
    "    \"\"\"\n",
    "    # keep tracking the loss\n",
    "    current_loss = 0\n",
    "    all_losses = []\n",
    "    rnn.train()\n",
    "    optimizer = torch.optim.SGD(rnn.parameters(), lr=learning_rate)\n",
    "\n",
    "    start = time.time()\n",
    "    print(f\"training on data set with n = {len(training_data)}\")\n",
    "\n",
    "    for iter in range(1, n_epoch + 1):\n",
    "        # clear the gradients\n",
    "        rnn.zero_grad() \n",
    "\n",
    "        # create some minibatches\n",
    "        # we cannot use dataloaders because each of our names is a different length\n",
    "        batches = list(range(len(training_data)))\n",
    "        random.shuffle(batches)\n",
    "        batches = np.array_split(batches, len(batches)//n_batch_size)\n",
    "\n",
    "        for idx, batch in enumerate(batches):\n",
    "            batch_loss = 0\n",
    "            for i in batch:\n",
    "                (label_tensor, text_tensor, label, text) = training_data[i]\n",
    "                output = rnn.forward(text_tensor)\n",
    "                loss = criterion(output, label_tensor)\n",
    "                batch_loss += loss\n",
    "            \n",
    "            # optimize parameters\n",
    "            batch_loss.backward()\n",
    "            nn.utils.clip_grad_norm_(rnn.parameters(), 3)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            current_loss += batch_loss.item() / len(batch)\n",
    "        \n",
    "        all_losses.append(current_loss / len(batches))\n",
    "        if iter % report_every == 0:\n",
    "            print(f\"{iter} ({iter / n_epoch:.0%}): \\t average batch loss = {all_losses[-1]}\")\n",
    "        current_loss = 0\n",
    "\n",
    "    return all_losses\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on data set with n = 16060\n",
      "5 (12%): \t average batch loss = 0.27426319316854847\n",
      "10 (25%): \t average batch loss = 0.239960134016092\n",
      "15 (38%): \t average batch loss = 0.215743846100798\n",
      "20 (50%): \t average batch loss = 0.20059040319621563\n",
      "25 (62%): \t average batch loss = 0.19230738061207991\n",
      "30 (75%): \t average batch loss = 0.1802687315200384\n",
      "35 (88%): \t average batch loss = 0.1791189535798935\n",
      "40 (100%): \t average batch loss = 0.17775440499851336\n",
      "training took 195.615092754364s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "all_losses = train(rnn, train_set, n_epoch=40, learning_rate=0.1, report_every=5)\n",
    "end = time.time()\n",
    "print(f\"training took {end-start}s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
