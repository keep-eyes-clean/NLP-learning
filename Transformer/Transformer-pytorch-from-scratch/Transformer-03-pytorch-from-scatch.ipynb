{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Attention is all you need\" 复现\n",
    "基于pytorch库的内容，先实现transformer的基本训练、评估和测试的全流程\n",
    "\n",
    "之后再尝试模仿他人，学习如何不完全借用pytorch的transformer从而实现语言翻译"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 数据处理代码优化 PreprocessData\n",
    "数据处理过程：\n",
    "- 1. load_corpus_generator, 原始语料库加载函数\n",
    "    - 支持语句长度限制\n",
    "- 2. TokenizerTrain class, 分词器训练类，用于训练本地语料库\n",
    "    - 逻辑稍后整理\n",
    "- 3. TokenizerLoader，加载训练好的分词器\n",
    "    - 逻辑...\n",
    "- 4. TranslationDataset class，数据集处理类\n",
    "- 5. collate_fn 函数，对数据进行批处理，包括填充、堆叠、排序等操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "config = {\n",
    "    'source-file':\"/harddisk1/SZC-Project/NLP-learning/Transformer/Transformer-pytorch-from-scratch/europarl-v7.de-en.en\",\n",
    "    'target-file':\"/harddisk1/SZC-Project/NLP-learning/Transformer/Transformer-pytorch-from-scratch/europarl-v7.de-en.de\",\n",
    "    'source-tokenizer-file':\"/harddisk1/SZC-Project/NLP-learning/Transformer/Transformer-pytorch-from-scratch/en_tokenizer.json\",\n",
    "    'target-tokenizer-file':\"/harddisk1/SZC-Project/NLP-learning/Transformer/Transformer-pytorch-from-scratch/de_tokenizer.json\",\n",
    "    'special-tokne':[\"[PAD]\", \"[UNK]\", \"[BOS]\", \"[EOS]\"],\n",
    "    'vocab-size':30000,\n",
    "    'min-length':5,\n",
    "    'max-length':128,\n",
    "    'batch-size':64,\n",
    "    'sample-ratio':0.1,\n",
    "    'num-workers':4\n",
    "}\n",
    "\n",
    "# 2. 分词训练器\n",
    "class TokenizerTrain:\n",
    "    def __init__(self, vocab_size, special_tokens):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.special_tokens = sepcial_tokens\n",
    "\n",
    "    def train_and_save(self, corpus, output_path, language_name):\n",
    "        tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "        tokenizer.pre_tokenizer = Whitespace()\n",
    "        trainer = BPE.Trainer(special_tokens=self.special_tokens, vocab_size=self.vocab_size)\n",
    "        tokenizer.train_from_iterator(corpus, trainer=trainer)\n",
    "        tokenizer.save(f\"{output_path}/{language_name}_tokenizer.json\")\n",
    "\n",
    "# 4. 数据集类\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, src_lines_path, tgt_lines_path, src_transformer_file, tgt_transformer_file, max_length):\n",
    "        self.src_generator = self.load_corpus_generator(src_lines_path, config['min-length'], config['max-length'])\n",
    "        self.tgt_generator = self.load_corpus_generator(tgt_lines_path, config['min-length'], config['max-length'])\n",
    "        self.src_lines = list(self.src_generator)\n",
    "        self.tgt_lines = list(self.tgt_generator)\n",
    "        self.src_transformer_tokenizer = self.TokenizerLoader(src_transformer_file)\n",
    "        self.tgt_transformer_tokenizer = self.TokenizerLoader(tgt_transformer_file)\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.src_lines)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        src_line = self.src_lines[idx]\n",
    "        tgt_line = self.tgt_lines[idx]\n",
    "\n",
    "        src_encoding = self.src_transformer_tokenizer(\n",
    "            src_line,\n",
    "            padding = 'max_length',\n",
    "            truncation = True,\n",
    "            max_length = self.max_length,\n",
    "            return_tensors = \"pt\"\n",
    "        )\n",
    "\n",
    "        tgt_encoding = self.tgt_transformer_tokenizer(\n",
    "            tgt_line,\n",
    "            padding = 'max_length',\n",
    "            truncation = True,\n",
    "            max_length = self.max_length,\n",
    "            return_tensors = \"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": src_encoding['input_ids'].squeeze(0),\n",
    "            \"attention_mask\": src_encoding['attention_mask'].squeeze(0),\n",
    "            \"labels\":tgt_encoding['input_ids'].squeeze(0)\n",
    "        }\n",
    "\n",
    "    def load_corpus_generator(self, file_path, min_length=5, max_length=128):\n",
    "        #output = []\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line and min_length <= len(line.split()) <= max_length:\n",
    "                    #output.append(line)\n",
    "                    yield line\n",
    "\n",
    "    def TokenizerLoader(self, tokenizer_path):\n",
    "        tokenizer = Tokenizer.from_file(tokenizer_path)\n",
    "        tokenizer = PreTrainedTokenizerFast(\n",
    "            tokenizer_object = tokenizer,\n",
    "            bos_token = \"[BOS]\",\n",
    "            eos_token = \"[EOS]\",\n",
    "            pad_token = \"[PAD]\",\n",
    "            unk_token = \"[UNK]\"\n",
    "        )\n",
    "        return tokenizer\n",
    "\n",
    "# 自定义 collate_fn\n",
    "def collate_fn(batch):\n",
    "    input_ids = pad_sequence([item['input_ids'] for item in batch], batch_first=True, padding_value=0)\n",
    "    attention_mask = pad_sequence([item['attention_mask'] for item in batch], batch_first=True, padding_value=0)\n",
    "    labels = pad_sequence([item['labels'] for item in batch], batch_first=True, padding_value=-100)  # -100 是常用的忽略索引值\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 测试用例\n",
    "translation_dataset = TranslationDataset(config['source-file'], config['target-file'], config['source-tokenizer-file'], config['target-tokenizer-file'], config['max-length'])\n",
    "\n",
    "\n",
    "indices = np.random.choice(len(translation_dataset), int(len(translation_dataset) * config[\"sample-ratio\"]), replace=False)\n",
    "sampler = SubsetRandomSampler(indices)\n",
    "\n",
    "sampled_loader = DataLoader(\n",
    "    translation_dataset,\n",
    "    batch_size=config['batch-size'],\n",
    "    sampler=sampler,\n",
    "    num_workers=config['num-workers'],\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 经过通义优化后的代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "class TokenizerTrainer:\n",
    "    \"\"\"\n",
    "    训练并保存分词器的类。\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, special_tokens):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.special_tokens = special_tokens\n",
    "\n",
    "    def train_and_save(self, corpus_generator, output_path, language_name):\n",
    "        \"\"\"\n",
    "        训练分词器并保存为 JSON 文件。\n",
    "        :param corpus_generator: 语料库生成器，逐行生成句子。\n",
    "        :param output_path: 输出路径。\n",
    "        :param language_name: 语言名称，用于文件命名。\n",
    "        \"\"\"\n",
    "        tokenizer = Tokenizer(models.BPE(unk_token=self.special_tokens[1]))  # 使用 BPE 模型\n",
    "        tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()  # 添加预分词器\n",
    "        trainer = trainers.BpeTrainer(special_tokens=self.special_tokens, vocab_size=self.vocab_size)\n",
    "        tokenizer.train_from_iterator(corpus_generator, trainer=trainer)\n",
    "        tokenizer.save(f\"{output_path}/{language_name}_tokenizer.json\")\n",
    "\n",
    "    def create_pair_file(self, source_file, target_file, pair_file):\n",
    "        \"\"\"\n",
    "        创建源语言和目标语言的配对文件。\n",
    "        :param source_file: 源语言文件路径。\n",
    "        :param target_file: 目标语言文件路径。\n",
    "        :param pair_file: 输出的配对文件路径。\n",
    "        \"\"\"\n",
    "        with open(source_file, 'r', encoding='utf-8') as src_f, \\\n",
    "            open(target_file, 'r', encoding='utf-8') as tgt_f, \\\n",
    "            open(pair_file, 'w', encoding='utf-8') as pair_f:\n",
    "\n",
    "            for src_line, tgt_line in zip(src_f, tgt_f):\n",
    "                src_line = src_line.strip()\n",
    "                tgt_line = tgt_line.strip()\n",
    "                if src_line and tgt_line:  # 确保句子非空\n",
    "                    pair_f.write(f\"{src_line}\\t{tgt_line}\\n\")\n",
    "\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    翻译任务的数据集类。\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        \"\"\"\n",
    "        初始化数据集。\n",
    "        :param src_file: 源语言文件路径。\n",
    "        :param tgt_file: 目标语言文件路径。\n",
    "        :param src_tokenizer_file: 源语言分词器文件路径。\n",
    "        :param tgt_tokenizer_file: 目标语言分词器文件路径。\n",
    "        :param config: 配置字典，包含 min_length, max_length, max_length 等参数。\n",
    "        \"\"\"\n",
    "        self.src_lines, self.tgt_lines = self.load_pairs(config['pair-file'], config['min-length'], config['max-length'])\n",
    "        assert len(self.src_lines) == len(self.tgt_lines), \"源语言和目标语言的句子数量不匹配！\"\n",
    "\n",
    "        # 加载分词器\n",
    "        self.src_tokenizer = self.load_tokenizer(config['source-tokenizer-file'])\n",
    "        self.tgt_tokenizer = self.load_tokenizer(config['target-tokenizer-file'])\n",
    "\n",
    "        self.max_length = config['max-length']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_lines)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_line = self.src_lines[idx]\n",
    "        tgt_line = self.tgt_lines[idx]\n",
    "\n",
    "        # 编码源语言和目标语言\n",
    "        src_encoding = self.src_tokenizer(\n",
    "            src_line,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        tgt_encoding = self.tgt_tokenizer(\n",
    "            tgt_line,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": src_encoding['input_ids'].squeeze(0),\n",
    "            \"attention_mask\": src_encoding['attention_mask'].squeeze(0),\n",
    "            \"labels\": tgt_encoding['input_ids'].squeeze(0)\n",
    "        }\n",
    "\n",
    "    def load_corpus(self, file_path, min_length, max_length):\n",
    "        \"\"\"\n",
    "        加载语料库，过滤掉不符合长度要求的句子。\n",
    "        :param file_path: 文件路径。\n",
    "        :param min_length: 最小句子长度。\n",
    "        :param max_length: 最大句子长度。\n",
    "        :return: 过滤后的句子生成器。\n",
    "        \"\"\"\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line and min_length <= len(line.split()) <= max_length:\n",
    "                    yield line\n",
    "\n",
    "    def load_tokenizer(self, tokenizer_path):\n",
    "        \"\"\"\n",
    "        加载分词器并封装为 PreTrainedTokenizerFast 对象。\n",
    "        :param tokenizer_path: 分词器文件路径。\n",
    "        :return: 加载好的分词器。\n",
    "        \"\"\"\n",
    "        tokenizer = Tokenizer.from_file(tokenizer_path)\n",
    "        return PreTrainedTokenizerFast(\n",
    "            tokenizer_object=tokenizer,\n",
    "            bos_token=\"[BOS]\",\n",
    "            eos_token=\"[EOS]\",\n",
    "            pad_token=\"[PAD]\",\n",
    "            unk_token=\"[UNK]\"\n",
    "        )\n",
    "    \n",
    "    def load_pairs(self, pair_file, min_length, max_length):\n",
    "        \"\"\"\n",
    "        加载配对文件并过滤掉不符合长度要求的句子对。\n",
    "        :param pair_file: 配对文件路径。\n",
    "        :param min_length: 最小句子长度。\n",
    "        :param max_length: 最大句子长度。\n",
    "        :return: 过滤后的源语言和目标语言句子列表。\n",
    "        \"\"\"\n",
    "        src_lines, tgt_lines = [], []\n",
    "        with open(pair_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split('\\t')\n",
    "                if len(parts) != 2:\n",
    "                    continue  # 跳过格式错误的行\n",
    "\n",
    "                src_line, tgt_line = parts[0], parts[1]\n",
    "                if (min_length <= len(src_line.split()) <= max_length and\n",
    "                        min_length <= len(tgt_line.split()) <= max_length):\n",
    "                    src_lines.append(src_line)\n",
    "                    tgt_lines.append(tgt_line)\n",
    "\n",
    "        return src_lines, tgt_lines\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = pad_sequence([item['input_ids'] for item in batch], batch_first=True, padding_value=0)\n",
    "    attention_mask = pad_sequence([item['attention_mask'] for item in batch], batch_first=True, padding_value=0)\n",
    "    labels = pad_sequence([item['labels'] for item in batch], batch_first=True, padding_value=-100)  # -100 是常用的忽略索引值\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "# 配置字典\n",
    "config = {\n",
    "    'pair-file': \"/harddisk1/SZC-Project/NLP-learning/Transformer/Transformer-pytorch-from-scratch/en-de-pair.txt\",\n",
    "    'source-file': \"/harddisk1/SZC-Project/NLP-learning/Transformer/Transformer-pytorch-from-scratch/europarl-v7.de-en.en\",\n",
    "    'target-file': \"/harddisk1/SZC-Project/NLP-learning/Transformer/Transformer-pytorch-from-scratch/europarl-v7.de-en.de\",\n",
    "    'source-tokenizer-file': \"/harddisk1/SZC-Project/NLP-learning/Transformer/Transformer-pytorch-from-scratch/en_tokenizer.json\",\n",
    "    'target-tokenizer-file': \"/harddisk1/SZC-Project/NLP-learning/Transformer/Transformer-pytorch-from-scratch/de_tokenizer.json\",\n",
    "    'special-tokens': [\"[PAD]\", \"[UNK]\", \"[BOS]\", \"[EOS]\"],\n",
    "    'vocab-size': 30000,\n",
    "    'min-length': 5,\n",
    "    'max-length': 128,\n",
    "    'batch-size': 64,\n",
    "    'sample-ratio': 0.1,\n",
    "    'num-workers': 4\n",
    "}\n",
    "\n",
    "# 测试用例\n",
    "if __name__ == \"__main__\":\n",
    "    # 创建数据集\n",
    "    tokenizerTrainer = TokenizerTrainer(config['vocab-size'], config['special-tokens'])\n",
    "    tokenizerTrainer.create_pair_file(source_file=config['source-file'], target_file=config['target-file'], pair_file='en-de-pair.txt')\n",
    "\n",
    "    translation_dataset = TranslationDataset(config)\n",
    "\n",
    "    # 创建采样器\n",
    "    indices = np.random.choice(len(translation_dataset), int(len(translation_dataset) * config[\"sample-ratio\"]), replace=False)\n",
    "    sampler = SubsetRandomSampler(indices)\n",
    "\n",
    "    # 创建数据加载器\n",
    "    sampled_loader = DataLoader(\n",
    "        translation_dataset,\n",
    "        batch_size=config['batch-size'],\n",
    "        sampler=sampler,\n",
    "        num_workers=config['num-workers'],\n",
    "        collate_fn=collate_fn\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  11, 1934,   12,  ...,    0,    0,    0],\n",
      "        [  44,  465,  842,  ...,    0,    0,    0],\n",
      "        [ 838,  317,  323,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [1100,  340,  317,  ...,    0,    0,    0],\n",
      "        [1754,  422, 2414,  ...,    0,    0,    0],\n",
      "        [ 584,  317,   67,  ...,    0,    0,    0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[ 6744,   766, 13879,  ...,     0,     0,     0],\n",
      "        [ 2726,  1757,  1055,  ...,     0,     0,     0],\n",
      "        [  622,   371,   377,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [ 1018,   556,  7595,  ...,     0,     0,     0],\n",
      "        [  557,   416,  3325,  ...,     0,     0,     0],\n",
      "        [ 1005,   371, 29905,  ...,     0,     0,     0]])}\n"
     ]
    }
   ],
   "source": [
    "for batch in sampled_loader:\n",
    "    print(batch)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
