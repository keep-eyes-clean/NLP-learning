{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Attention is all you need\" 复现\n",
    "基于pytorch库的内容，先实现transformer的基本训练、评估和测试的全流程\n",
    "\n",
    "之后再尝试模仿他人，学习如何不完全借用pytorch的transformer从而实现语言翻译"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 数据处理代码优化 PreprocessData\n",
    "数据处理过程：\n",
    "- 1. load_corpus_generator, 原始语料库加载函数\n",
    "    - 支持语句长度限制\n",
    "- 2. TokenizerTrain class, 分词器训练类，用于训练本地语料库\n",
    "    - 逻辑稍后整理\n",
    "- 3. TokenizerLoader，加载训练好的分词器\n",
    "    - 逻辑...\n",
    "- 4. TranslationDataset class，数据集处理类\n",
    "- 5. collate_fn 函数，对数据进行批处理，包括填充、堆叠、排序等操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "config = {\n",
    "    'source-file':\"/harddisk1/SZC-Project/NLP-learning/Transformer/Transformer-pytorch-from-scratch/europarl-v7.de-en.en\",\n",
    "    'target-file':\"/harddisk1/SZC-Project/NLP-learning/Transformer/Transformer-pytorch-from-scratch/europarl-v7.de-en.de\",\n",
    "    'source-tokenizer-file':\"/harddisk1/SZC-Project/NLP-learning/Transformer/Transformer-pytorch-from-scratch/en_tokenizer.json\",\n",
    "    'target-tokenizer-file':\"/harddisk1/SZC-Project/NLP-learning/Transformer/Transformer-pytorch-from-scratch/de_tokenizer.json\",\n",
    "    'special-tokne':[\"[PAD]\", \"[UNK]\", \"[BOS]\", \"[EOS]\"],\n",
    "    'vocab-size':30000,\n",
    "    'min-length':5,\n",
    "    'max-length':128,\n",
    "    'batch-size':64,\n",
    "    'sample-ratio':0.1,\n",
    "    'num-workers':4\n",
    "}\n",
    "\n",
    "# 1. 加载符合要求的语料库\n",
    "def load_corpus_generator(file_path, min_length=5, max_length=128):\n",
    "    #output = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line and min_length <= len(line.split()) <= max_length:\n",
    "                #output.append(line)\n",
    "                yield line\n",
    "    #return output\n",
    "\n",
    "# 2. 分词训练器\n",
    "class TokenizerTrain:\n",
    "    def __init__(self, vocab_size, special_tokens):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.special_tokens = sepcial_tokens\n",
    "\n",
    "    def train_and_save(self, corpus, output_path, language_name):\n",
    "        tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "        tokenizer.pre_tokenizer = Whitespace()\n",
    "        trainer = BPE.Trainer(special_tokens=self.special_tokens, vocab_size=self.vocab_size)\n",
    "        tokenizer.train_from_iterator(corpus, trainer=trainer)\n",
    "        tokenizer.save(f\"{output_path}/{language_name}_tokenizer.json\")\n",
    "\n",
    "# 3. 加载分词器\n",
    "def TokenizerLoader(tokenizer_path):\n",
    "    tokenizer = Tokenizer.from_file(tokenizer_path)\n",
    "    tokenizer = PreTrainedTokenizerFast(\n",
    "        tokenizer_object = tokenizer,\n",
    "        bos_token = \"[BOS]\",\n",
    "        eos_token = \"[EOS]\",\n",
    "        pad_token = \"[PAD]\",\n",
    "        unk_token = \"[UNK]\"\n",
    "    )\n",
    "    return tokenizer\n",
    "\n",
    "# 4. 数据集类\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, src_lines, tgt_lines, src_transformer_file, tgt_transformer_file, max_length):\n",
    "        self.src_generator = src_lines\n",
    "        self.tgt_generator = tgt_lines\n",
    "        self.src_lines = list(src_lines)\n",
    "        self.tgt_lines = list(tgt_lines)\n",
    "        self.src_transformer_tokenizer = TokenizerLoader(src_transformer_file)\n",
    "        self.tgt_transformer_tokenizer = TokenizerLoader(tgt_transformer_file)\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.src_lines)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        src_line = self.src_lines[idx]\n",
    "        tgt_line = self.tgt_lines[idx]\n",
    "\n",
    "        src_encoding = self.src_transformer_tokenizer(\n",
    "            src_line,\n",
    "            padding = 'max_length',\n",
    "            truncation = True,\n",
    "            max_length = self.max_length,\n",
    "            return_tensors = \"pt\"\n",
    "        )\n",
    "\n",
    "        tgt_encoding = self.tgt_transformer_tokenizer(\n",
    "            tgt_line,\n",
    "            padding = 'max_length',\n",
    "            truncation = True,\n",
    "            max_length = self.max_length,\n",
    "            return_tensors = \"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": src_encoding['input_ids'].squeeze(0),\n",
    "            \"attention_mask\": src_encoding['attention_mask'].squeeze(0),\n",
    "            \"labels\":tgt_encoding['input_ids'].squeeze(0)\n",
    "        }\n",
    "\n",
    "# 自定义 collate_fn\n",
    "def collate_fn(batch):\n",
    "    input_ids = pad_sequence([item['input_ids'] for item in batch], batch_first=True, padding_value=0)\n",
    "    attention_mask = pad_sequence([item['attention_mask'] for item in batch], batch_first=True, padding_value=0)\n",
    "    labels = pad_sequence([item['labels'] for item in batch], batch_first=True, padding_value=-100)  # -100 是常用的忽略索引值\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "# 测试用例\n",
    "src_lines = load_corpus_generator(config['source-file'], config['min-length'], config['max-length'])\n",
    "tgt_lines = load_corpus_generator(config['source-file'], config['min-length'], config['max-length'])\n",
    "translation_dataset = TranslationDataset(src_lines, tgt_lines, config['source-tokenizer-file'], config['target-tokenizer-file'], config['max-length'])\n",
    "\n",
    "\n",
    "indices = np.random.choice(len(translation_dataset), int(len(translation_dataset) * config[\"sample-ratio\"]), replace=False)\n",
    "sampler = SubsetRandomSampler(indices)\n",
    "\n",
    "sampled_loader = DataLoader(\n",
    "    translation_dataset,\n",
    "    batch_size=config['batch-size'],\n",
    "    sampler=sampler,\n",
    "    num_workers=config['num-workers'],\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2912"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sampled_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in sampled_loader:\n",
    "    tmp = batch\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 128])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp['input_ids'].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 基于pytorch的Transformer模型搭建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
