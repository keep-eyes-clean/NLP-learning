{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Attention is all you need\" 复现\n",
    "基于pytorch库的内容，先实现transformer的基本训练、评估和测试的全流程\n",
    "\n",
    "之后再尝试模仿他人，学习如何不完全借用pytorch的transformer从而实现语言翻译"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 数据处理代码优化 PreprocessData\n",
    "数据处理过程：\n",
    "- 1. load_corpus_generator, 原始语料库加载函数\n",
    "    - 支持语句长度限制\n",
    "- 2. TokenizerTrain class, 分词器训练类，用于训练本地语料库\n",
    "    - 逻辑稍后整理\n",
    "- 3. TokenizerLoader，加载训练好的分词器\n",
    "    - 逻辑...\n",
    "- 4. TranslationDataset class，数据集处理类\n",
    "- 5. collate_fn 函数，对数据进行批处理，包括填充、堆叠、排序等操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "config = {\n",
    "    'source-file':\"/harddisk1/SZC-Project/NLP-learning/Transformer/Transformer-pytorch-from-scratch/europarl-v7.de-en.en\",\n",
    "    'target-file':\"/harddisk1/SZC-Project/NLP-learning/Transformer/Transformer-pytorch-from-scratch/europarl-v7.de-en.de\",\n",
    "    'source-tokenizer-file':\"/harddisk1/SZC-Project/NLP-learning/Transformer/Transformer-pytorch-from-scratch/en_tokenizer.json\",\n",
    "    'target-tokenizer-file':\"/harddisk1/SZC-Project/NLP-learning/Transformer/Transformer-pytorch-from-scratch/de_tokenizer.json\",\n",
    "    'special-tokne':[\"[PAD]\", \"[UNK]\", \"[BOS]\", \"[EOS]\"],\n",
    "    'vocab-size':30000,\n",
    "    'min-length':5,\n",
    "    'max-length':128,\n",
    "    'batch-size':64,\n",
    "    'sample-ratio':0.1,\n",
    "    'num-workers':4\n",
    "}\n",
    "\n",
    "# 2. 分词训练器\n",
    "class TokenizerTrain:\n",
    "    def __init__(self, vocab_size, special_tokens):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.special_tokens = sepcial_tokens\n",
    "\n",
    "    def train_and_save(self, corpus, output_path, language_name):\n",
    "        tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "        tokenizer.pre_tokenizer = Whitespace()\n",
    "        trainer = BPE.Trainer(special_tokens=self.special_tokens, vocab_size=self.vocab_size)\n",
    "        tokenizer.train_from_iterator(corpus, trainer=trainer)\n",
    "        tokenizer.save(f\"{output_path}/{language_name}_tokenizer.json\")\n",
    "\n",
    "# 4. 数据集类\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, src_lines_path, tgt_lines_path, src_transformer_file, tgt_transformer_file, max_length):\n",
    "        self.src_generator = self.load_corpus_generator(src_lines_path, config['min-length'], config['max-length'])\n",
    "        self.tgt_generator = self.load_corpus_generator(tgt_lines_path, config['min-length'], config['max-length'])\n",
    "        self.src_lines = list(self.src_generator)\n",
    "        self.tgt_lines = list(self.tgt_generator)\n",
    "        self.src_transformer_tokenizer = self.TokenizerLoader(src_transformer_file)\n",
    "        self.tgt_transformer_tokenizer = self.TokenizerLoader(tgt_transformer_file)\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.src_lines)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        src_line = self.src_lines[idx]\n",
    "        tgt_line = self.tgt_lines[idx]\n",
    "\n",
    "        src_encoding = self.src_transformer_tokenizer(\n",
    "            src_line,\n",
    "            padding = 'max_length',\n",
    "            truncation = True,\n",
    "            max_length = self.max_length,\n",
    "            return_tensors = \"pt\"\n",
    "        )\n",
    "\n",
    "        tgt_encoding = self.tgt_transformer_tokenizer(\n",
    "            tgt_line,\n",
    "            padding = 'max_length',\n",
    "            truncation = True,\n",
    "            max_length = self.max_length,\n",
    "            return_tensors = \"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": src_encoding['input_ids'].squeeze(0),\n",
    "            \"attention_mask\": src_encoding['attention_mask'].squeeze(0),\n",
    "            \"labels\":tgt_encoding['input_ids'].squeeze(0)\n",
    "        }\n",
    "\n",
    "    def load_corpus_generator(self, file_path, min_length=5, max_length=128):\n",
    "        #output = []\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line and min_length <= len(line.split()) <= max_length:\n",
    "                    #output.append(line)\n",
    "                    yield line\n",
    "\n",
    "    def TokenizerLoader(self, tokenizer_path):\n",
    "        tokenizer = Tokenizer.from_file(tokenizer_path)\n",
    "        tokenizer = PreTrainedTokenizerFast(\n",
    "            tokenizer_object = tokenizer,\n",
    "            bos_token = \"[BOS]\",\n",
    "            eos_token = \"[EOS]\",\n",
    "            pad_token = \"[PAD]\",\n",
    "            unk_token = \"[UNK]\"\n",
    "        )\n",
    "        return tokenizer\n",
    "\n",
    "# 自定义 collate_fn\n",
    "def collate_fn(batch):\n",
    "    input_ids = pad_sequence([item['input_ids'] for item in batch], batch_first=True, padding_value=0)\n",
    "    attention_mask = pad_sequence([item['attention_mask'] for item in batch], batch_first=True, padding_value=0)\n",
    "    labels = pad_sequence([item['labels'] for item in batch], batch_first=True, padding_value=-100)  # -100 是常用的忽略索引值\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 测试用例\n",
    "translation_dataset = TranslationDataset(config['source-file'], config['target-file'], config['source-tokenizer-file'], config['target-tokenizer-file'], config['max-length'])\n",
    "\n",
    "\n",
    "indices = np.random.choice(len(translation_dataset), int(len(translation_dataset) * config[\"sample-ratio\"]), replace=False)\n",
    "sampler = SubsetRandomSampler(indices)\n",
    "\n",
    "sampled_loader = DataLoader(\n",
    "    translation_dataset,\n",
    "    batch_size=config['batch-size'],\n",
    "    sampler=sampler,\n",
    "    num_workers=config['num-workers'],\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 经过通义优化后的代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "class TokenizerTrainer:\n",
    "    \"\"\"\n",
    "    训练并保存分词器的类。\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, special_tokens):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.special_tokens = special_tokens\n",
    "\n",
    "    def train_and_save(self, corpus_generator, output_path, language_name):\n",
    "        \"\"\"\n",
    "        训练分词器并保存为 JSON 文件。\n",
    "        :param corpus_generator: 语料库生成器，逐行生成句子。\n",
    "        :param output_path: 输出路径。\n",
    "        :param language_name: 语言名称，用于文件命名。\n",
    "        \"\"\"\n",
    "        tokenizer = Tokenizer(models.BPE(unk_token=self.special_tokens[1]))  # 使用 BPE 模型\n",
    "        tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()  # 添加预分词器\n",
    "        trainer = trainers.BpeTrainer(special_tokens=self.special_tokens, vocab_size=self.vocab_size)\n",
    "        tokenizer.train_from_iterator(corpus_generator, trainer=trainer)\n",
    "        tokenizer.save(f\"{output_path}/{language_name}_tokenizer.json\")\n",
    "\n",
    "    def create_pair_file(self, source_file, target_file, pair_file):\n",
    "        \"\"\"\n",
    "        创建源语言和目标语言的配对文件。\n",
    "        :param source_file: 源语言文件路径。\n",
    "        :param target_file: 目标语言文件路径。\n",
    "        :param pair_file: 输出的配对文件路径。\n",
    "        \"\"\"\n",
    "        with open(source_file, 'r', encoding='utf-8') as src_f, \\\n",
    "            open(target_file, 'r', encoding='utf-8') as tgt_f, \\\n",
    "            open(pair_file, 'w', encoding='utf-8') as pair_f:\n",
    "\n",
    "            for src_line, tgt_line in zip(src_f, tgt_f):\n",
    "                src_line = src_line.strip()\n",
    "                tgt_line = tgt_line.strip()\n",
    "                if src_line and tgt_line:  # 确保句子非空\n",
    "                    pair_f.write(f\"{src_line}\\t{tgt_line}\\n\")\n",
    "\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    翻译任务的数据集类。\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        \"\"\"\n",
    "        初始化数据集。\n",
    "        :param src_file: 源语言文件路径。\n",
    "        :param tgt_file: 目标语言文件路径。\n",
    "        :param src_tokenizer_file: 源语言分词器文件路径。\n",
    "        :param tgt_tokenizer_file: 目标语言分词器文件路径。\n",
    "        :param config: 配置字典，包含 min_length, max_length, max_length 等参数。\n",
    "        \"\"\"\n",
    "        self.src_lines, self.tgt_lines = self.load_pairs(config['pair-file'], config['min-length'], config['max-length'])\n",
    "        assert len(self.src_lines) == len(self.tgt_lines), \"源语言和目标语言的句子数量不匹配！\"\n",
    "\n",
    "        # 加载分词器\n",
    "        self.src_tokenizer = self.load_tokenizer(config['source-tokenizer-file'])\n",
    "        self.tgt_tokenizer = self.load_tokenizer(config['target-tokenizer-file'])\n",
    "\n",
    "        self.max_length = config['max-length']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_lines)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_line = self.src_lines[idx]\n",
    "        tgt_line = self.tgt_lines[idx]\n",
    "\n",
    "        # 编码源语言和目标语言\n",
    "        src_encoding = self.src_tokenizer(\n",
    "            src_line,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        tgt_encoding = self.tgt_tokenizer(\n",
    "            tgt_line,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": src_encoding['input_ids'].squeeze(0),\n",
    "            \"attention_mask\": src_encoding['attention_mask'].squeeze(0),\n",
    "            \"labels\": tgt_encoding['input_ids'].squeeze(0)\n",
    "        }\n",
    "\n",
    "    def load_corpus(self, file_path, min_length, max_length):\n",
    "        \"\"\"\n",
    "        加载语料库，过滤掉不符合长度要求的句子。\n",
    "        :param file_path: 文件路径。\n",
    "        :param min_length: 最小句子长度。\n",
    "        :param max_length: 最大句子长度。\n",
    "        :return: 过滤后的句子生成器。\n",
    "        \"\"\"\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line and min_length <= len(line.split()) <= max_length:\n",
    "                    yield line\n",
    "\n",
    "    def load_tokenizer(self, tokenizer_path):\n",
    "        \"\"\"\n",
    "        加载分词器并封装为 PreTrainedTokenizerFast 对象。\n",
    "        :param tokenizer_path: 分词器文件路径。\n",
    "        :return: 加载好的分词器。\n",
    "        \"\"\"\n",
    "        tokenizer = Tokenizer.from_file(tokenizer_path)\n",
    "        return PreTrainedTokenizerFast(\n",
    "            tokenizer_object=tokenizer,\n",
    "            bos_token=\"[BOS]\",\n",
    "            eos_token=\"[EOS]\",\n",
    "            pad_token=\"[PAD]\",\n",
    "            unk_token=\"[UNK]\"\n",
    "        )\n",
    "    \n",
    "    def load_pairs(self, pair_file, min_length, max_length):\n",
    "        \"\"\"\n",
    "        加载配对文件并过滤掉不符合长度要求的句子对。\n",
    "        :param pair_file: 配对文件路径。\n",
    "        :param min_length: 最小句子长度。\n",
    "        :param max_length: 最大句子长度。\n",
    "        :return: 过滤后的源语言和目标语言句子列表。\n",
    "        \"\"\"\n",
    "        src_lines, tgt_lines = [], []\n",
    "        with open(pair_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split('\\t')\n",
    "                if len(parts) != 2:\n",
    "                    continue  # 跳过格式错误的行\n",
    "\n",
    "                src_line, tgt_line = parts[0], parts[1]\n",
    "                if (min_length <= len(src_line.split()) <= max_length and\n",
    "                        min_length <= len(tgt_line.split()) <= max_length):\n",
    "                    src_lines.append(src_line)\n",
    "                    tgt_lines.append(tgt_line)\n",
    "\n",
    "        return src_lines, tgt_lines\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = pad_sequence([item['input_ids'] for item in batch], batch_first=True, padding_value=0)\n",
    "    attention_mask = pad_sequence([item['attention_mask'] for item in batch], batch_first=True, padding_value=0)\n",
    "    labels = pad_sequence([item['labels'] for item in batch], batch_first=True, padding_value=-100)  # -100 是常用的忽略索引值\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "# 配置字典\n",
    "config = {\n",
    "    'pair-file': \"/harddisk1/SZC-Project/NLP-learning/Transformer/Transformer-pytorch-from-scratch/en-de-pair.txt\",\n",
    "    'source-file': \"/harddisk1/SZC-Project/NLP-learning/Transformer/Transformer-pytorch-from-scratch/europarl-v7.de-en.en\",\n",
    "    'target-file': \"/harddisk1/SZC-Project/NLP-learning/Transformer/Transformer-pytorch-from-scratch/europarl-v7.de-en.de\",\n",
    "    'source-tokenizer-file': \"/harddisk1/SZC-Project/NLP-learning/Transformer/Transformer-pytorch-from-scratch/en_tokenizer.json\",\n",
    "    'target-tokenizer-file': \"/harddisk1/SZC-Project/NLP-learning/Transformer/Transformer-pytorch-from-scratch/de_tokenizer.json\",\n",
    "    'special-tokens': [\"[PAD]\", \"[UNK]\", \"[BOS]\", \"[EOS]\"],\n",
    "    'vocab-size': 30000,\n",
    "    'min-length': 5,\n",
    "    'max-length': 128,\n",
    "    'batch-size': 64,\n",
    "    'sample-ratio': 0.1,\n",
    "    'num-workers': 4\n",
    "}\n",
    "\n",
    "# 测试用例\n",
    "if __name__ == \"__main__\":\n",
    "    # 创建数据集\n",
    "    tokenizerTrainer = TokenizerTrainer(config['vocab-size'], config['special-tokens'])\n",
    "    tokenizerTrainer.create_pair_file(source_file=config['source-file'], target_file=config['target-file'], pair_file='en-de-pair.txt')\n",
    "\n",
    "    translation_dataset = TranslationDataset(config)\n",
    "\n",
    "    # 创建采样器\n",
    "    indices = np.random.choice(len(translation_dataset), int(len(translation_dataset) * config[\"sample-ratio\"]), replace=False)\n",
    "    sampler = SubsetRandomSampler(indices)\n",
    "\n",
    "    # 创建数据加载器\n",
    "    sampled_loader = DataLoader(\n",
    "        translation_dataset,\n",
    "        batch_size=config['batch-size'],\n",
    "        sampler=sampler,\n",
    "        num_workers=config['num-workers'],\n",
    "        collate_fn=collate_fn\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  11, 1934,   12,  ...,    0,    0,    0],\n",
      "        [  44,  465,  842,  ...,    0,    0,    0],\n",
      "        [ 838,  317,  323,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [1100,  340,  317,  ...,    0,    0,    0],\n",
      "        [1754,  422, 2414,  ...,    0,    0,    0],\n",
      "        [ 584,  317,   67,  ...,    0,    0,    0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[ 6744,   766, 13879,  ...,     0,     0,     0],\n",
      "        [ 2726,  1757,  1055,  ...,     0,     0,     0],\n",
      "        [  622,   371,   377,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [ 1018,   556,  7595,  ...,     0,     0,     0],\n",
      "        [  557,   416,  3325,  ...,     0,     0,     0],\n",
      "        [ 1005,   371, 29905,  ...,     0,     0,     0]])}\n"
     ]
    }
   ],
   "source": [
    "for batch in sampled_loader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型基础模块\n",
    "\n",
    "来源自：https://github.com/jadore801120/attention-is-all-you-need-pytorch/blob/master/transformer\n",
    "\n",
    "有简单的copy，也有复杂的修改和优化\n",
    "\n",
    "torch的tensor之间的计算还是不会"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    \"\"\" Scaled Dot-product Attention \n",
    "    q: (batch_size, num_heads, seq_len_q, d_k)\n",
    "    k: (batch_size, num_heads, seq_len_k, d_k)\n",
    "    v: (batch_size, num_heads, seq_len_v, d_v)\n",
    "\n",
    "    attn: (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "    output: (batch_size, num_heads, seq_len_q, d_v)\n",
    "    \"\"\"\n",
    "    def __init__(self, temperature, attn_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.dropout = nn.Dropout(attn_dropout)\n",
    "    \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        \"\"\"\n",
    "        计算 attn = torch.matmul(q / self.temperature, k.transpose(2, 3)) 时\n",
    "        k.transpose(2, 3) 改变 k 的大小为 (batch_size, num_heads, d_k, seq_len_k)\n",
    "        结果 attn 的大小为 (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        \"\"\"\n",
    "        attn = torch.matmul(q / self.temperature, k.transpose(2,3))\n",
    "\n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill(mask==0,-1e9)\n",
    "\n",
    "        attn = self.dropout(F.softmax(attn,dim=-1))\n",
    "        \"\"\"\n",
    "        v: (batch_size, num_heads, seq_len_v, d_v)\n",
    "       attn(batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        \"\"\"\n",
    "        output = torch.matmul(attn, v)\n",
    "\n",
    "        return output, attn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"多头注意力机制\"\"\"\n",
    "    \"\"\"定义这个多头注意力机制的类\"\"\"\n",
    "    def __init__(self, n_head, d_model, d_k, d_v, dropout=0.1):\n",
    "        \"\"\"\n",
    "        n_head  :注意力头的数量\n",
    "        d_model :输入和输出的维度,通常是嵌入向量的维度\n",
    "        d_k     :每个注意力头的查询和键的维度, 通常是 d_model 除以 n_head 的值\n",
    "        d_v     :每个注意力头的值的维度。\n",
    "        droput  :\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.n_head = n_head\n",
    "        self.d_k = d_K\n",
    "        self.d_v = d_v\n",
    "\n",
    "        self.w_qs = nn.Linear(d_model, n_head*d_k, bias=False)\n",
    "        self.w_ks = nn.Linear(d_model, n_head*d_k, bias=False)\n",
    "        self.w_vs = nn.Linear(d_model, n_head*d_v, bias=False)\n",
    "        self.fc = nn.Linear(n_head * d_v, d_model, bias=False) # 结合模型理解\n",
    "\n",
    "        self.attention = ScaledDotProductAttention(temperature=d_k ** 0.5)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6) # \n",
    "    \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        d_k, d_v, n_head = self.d_k, self.d_v, self.n_head\n",
    "        sz_b, len_q, len_k, len_v = q.size(0), q.size(1), k.size(1), v.size(1)\n",
    "\n",
    "        residual = q\n",
    "        \n",
    "        # Pass through the pre-attention projection: b x lq x (n*dv)\n",
    "        # Separate different heads: b x lq x n x dv\n",
    "        q = self.w_qs(q).view(sz_b, len_q, n_head, d_k)\n",
    "        k = self.w_ks(k).view(sz_b, len_k, n_head, d_k)\n",
    "        v = self.w_vs(v).view(sz_b, len_v, n_head, d_v)\n",
    "\n",
    "        # Transpose for attention dot product: b x n x lq x dv\n",
    "        q, k, v = q.transpose(1,2), k.transpose(1,2), v.transpose(1,2)\n",
    "        if mask is not None:    \n",
    "            mask = mask.unsqueeze(1) # For head axis broadcasting.\n",
    "        q, attn = self.attention(q,k,v,mask=mask)\n",
    "        \n",
    "        # Transpose to move the head dimension back: b x lq x n x dv\n",
    "        # Combine the last two dimensions to concatenate all the heads together: b x lq x (n*dv)\n",
    "        q = q.transpose(1,2).contiguous().view(sz_b, len_q, -1)\n",
    "        q = self.dropout(self.fc(q))\n",
    "        q += residual\n",
    "        q = self.layer_norm(q)\n",
    "\n",
    "        return q,attn\n",
    "\n",
    "\"\"\"不熟悉\"\"\"\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"\"\"一个标准的FNN模型\"\"\"\n",
    "    \"\"\"FNN的模型结构是什么\"\"\"\n",
    "    def __init__(self, d_in, d_hid, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.w_1 = nn.Linear(d_in, d_hid)\n",
    "        self.w_2 = nn.Linear(d_hid, d_in)\n",
    "        self.layer_norm = nn.LayerNorm(d_in, eps=1e-6)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.w_2(F.relu(self.w_1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x+= residual\n",
    "        x = self.layer_norm(x)\n",
    "        return x\n",
    "\n",
    "\"\"\"不熟悉\"\"\"\n",
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"MultiHeadAttention+FNN\"\"\"\n",
    "    def __init__(self, d_model, d_inner, n_head, d_k, d_v, dropout=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attention = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n",
    "        self.feed_forward = PositionwiseFeedForward(d_model, d_inner, dropout=dropout)\n",
    "\n",
    "    def forward(self, enc_input, self_attn_mask=None):\n",
    "        enc_output, enc_self_attn = self.self_attention(enc_input, enc_input, enc_input, mask=self_attn_mask)\n",
    "        enc_output = self.feed_forward(enc_output)\n",
    "        return enc_output, enc_self_attn\n",
    "\n",
    "\"\"\"更不熟悉了\"\"\"\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, d_inner, n_head, d_k, d_v, dropout=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.slf_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n",
    "        self.enc_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n",
    "        self.feed_forward = PositionwiseFeedForward(d_model, d_inner, dropout=dropout)\n",
    "    \n",
    "    def forward(self, dec_input, enc_output, slf_attn_mask=None, dec_enc_attn_mask=None):\n",
    "        dec_output, dec_slf_attn = self.slf_attn(dec_input, dec_input, dec_input, mask=slf_attn_mask)\n",
    "        dec_output, dec_enc_attn = self.enc_attn(dec_output, enc_output, enc_output, mask=dec_enc_attn_mask) # 因为当前的decode是掩盖的，所以使用enc_output作为value\n",
    "        dec_output = self.feed_forward(dec_output)\n",
    "        return dec_output, dec_slf_attn, dec_enc_attn\n",
    "\n",
    "\"\"\"构筑模型\"\"\"\n",
    "\n",
    "def get_pad_mask(seq, pad_idx):\n",
    "    \"\"\"非填充值的位置为True，填充值的位置为False。这个掩码用于在注意力机制中忽略填充值\"\"\"\n",
    "    return (seq != pad_idx).unsqueeze(-2)\n",
    "\n",
    "\"\"\"可视化它\"\"\"\n",
    "def get_subsequent_mask(seq):\n",
    "    \"\"\"屏蔽序列中当前时间步之后的所有时间步的信息\"\"\"\n",
    "    sz_b, len_s = seq.size()\n",
    "    subsequent_mask = (1 - torch.triu(torch.ones((1, len_s, len_s), device=seq.device), diagonal=1)).bool()\n",
    "    return subsequent_mask\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_in, d_hid, n_position=200):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        self.register_buffer('pos_table', self._get_sinusoid_encoding_table(n_position, d_hid))\n",
    "    \n",
    "    def _get_sinusoid_encoding_table(self, n_position, d_hid):\n",
    "        position = torch.arange(n_position).unsqueeze(1)  # [n_position, 1]\n",
    "        div_term = torch.pow(10000, torch.arange(0, d_hid, 2) / d_hid)\n",
    "        angle_rads = position / div_term # [n_position, d_hid // 2]\n",
    "\n",
    "        # 偶数维度用 sin，奇数维度用 cos\n",
    "        sinusoid_table = torch.zeros((n_position, d_hid))  # 初始化表格\n",
    "        sinusoid_table[:, 0::2] = torch.sin(angle_rads)  # 偶数维度\n",
    "        sinusoid_table[:, 1::2] = torch.cos(angle_rads)  # 奇数维度\n",
    "        \n",
    "        return sinusoid_table.unsqueeze(0)  # [1, n_position, d_hid]\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pos_table[:, :x.size(1)].clone().detach()\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, n_src_vocab, d_word_vec, n_layers, n_head, d_k, d_v, d_model, d_inner, pad_idx, dropout=0.1, n_position=200, scale_emb=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.src_word_emb = nn.Embedding(n_src_vocab, d_word_vec, padding_idx=pad_idx) # ? pad_idx\n",
    "        self.position_enc = PositionalEncoding(d_word_vec, n_position=n_position)\n",
    "        self.dropout = dropout\n",
    "        self.layer_stack = nn.ModuleList([EncoderLayer(d_model, d_inner, n_head, d_k, d_v, dropout=dropout) for _ in range(n_layers)])\n",
    "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.scale_emb = scale_emb\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, src_seq, src_mask, return_attn=False):\n",
    "        enc_slf_attn_list = []\n",
    "\n",
    "        enc_output = self.slf_word_emb(src_seq)\n",
    "        if self.scale_emb:\n",
    "            enc_output *= self.d_model ** 0.5 # ??\n",
    "        enc_output = self.dropout(self.position_enc(enc_output))\n",
    "        enc_output = self.layer_norm(enc_output) # ?? 结构是这样子的吗\n",
    "        \n",
    "        for enc_layer in self.layer_stack:\n",
    "            enc_output, enc_slf_attn = enc_layer(enc_output, self_attn_mask=src_mask)\n",
    "            enc_self_attn_list += [enc_slf_attn] if return_attn else []\n",
    "        \n",
    "        if return_attn:\n",
    "            return enc_output, enc_self_attn_list\n",
    "        else:\n",
    "            return enc_output\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, n_tgt_vocab, d_word_vec, n_layers, d_k, d_v, d_model, d_inner, pad_idx, n_position=200, dropout=0.1, scale_emb=False):\n",
    "        super().__init__()\n",
    "        self.tgt_word_emb = nn.Embedding(n_tgt_vocab, d_word_vec, padding_idx=pad_idx)\n",
    "        self.position_enc = PositionalEncoding(d_word_vec, n_position=n_position)\n",
    "        self.dropout = dropout\n",
    "        self.layer_stack = nn.ModuleList([DecoderLayer(d_model, d_inner, n_head, d_k, d_v, dropout=dropout) for _ in range(n_layers)])\n",
    "        self.layer_norm  = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.scale_emb = scale_emb\n",
    "        self.d_model = d_model \n",
    "    \n",
    "    def forward(self, tgt_seq, tgt_mask, enc_output, src_mask, return_attn=False):\n",
    "        dec_slf_attn_list = []\n",
    "        dec_enc_attn_list = []\n",
    "\n",
    "        dec_output =  self.tgt_word_emb(tgt_seq)\n",
    "        if self.scale_emd:\n",
    "            dec_output *= self.d_model ** 0.5\n",
    "        dec_output = self.dropout(self.position_enc(dec_output))\n",
    "        dec_output = self.layer_norm(dec_output)\n",
    "\n",
    "        for dec_layer in self.layer_stack:\n",
    "            dec_output, dec_slf_attn, dec_enc_attn = dec_layer(dec_output, enc_output, slf_attn_mask=tgt_mask, dec_enc_attn_mask=src_mask)\n",
    "            dec_slf_attn_list += [dec_slf_attn] if return_attn else []\n",
    "            dec_enc_attn_list += [dec_enc_attn] if return_attn else []\n",
    "        \n",
    "        if return_attn:\n",
    "            return dec_output, dec_slf_attn_list, dec_enc_attn_list\n",
    "        else:\n",
    "            return dec_output\n",
    "    \n",
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        n_src_vocab, \n",
    "        n_tgt_vocab, \n",
    "        src_pad_idx, \n",
    "        tgt_pad_idx, \n",
    "        d_word_vec=512, \n",
    "        d_model=512, \n",
    "        d_inner=2048, \n",
    "        n_layers=6, \n",
    "        n_head=8, \n",
    "        d_k=64, \n",
    "        d_v=64, \n",
    "        dropout=0.1, \n",
    "        n_position=200, \n",
    "        tgt_emb_prj_weight_sharing=True, \n",
    "        emb_src_tgt_weight_sharing=True, \n",
    "        scale_emb_or_prj='prj'):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.tgt_pad_idx = tgt_pad_idx\n",
    "        assert scale_emb_or_prj in ['emb', 'prj', 'none']\n",
    "        scale_emb = (scale_emb_or_prj == 'emb') if tgt_emb_prj_weight_sharing else False\n",
    "        self.scale_prj = (scale_emb_or_prj == 'prj') if trg_emb_prj_weight_sharing else False\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "            n_src_vocab=n_src_vocab,\n",
    "            n_position=n_position,\n",
    "            d_word_vec=d_word_vec,\n",
    "            d_model=d_model,\n",
    "            d_inner=d_inner,\n",
    "            n_layers=n_layers,\n",
    "            n_head=n_head,\n",
    "            d_k=d_k,\n",
    "            d_v=d_v,\n",
    "            pad_idx=src_pad_idx,\n",
    "            dropout=dropout,\n",
    "            scale_emb=scale_emb\n",
    "        )\n",
    "\n",
    "        self.decoder = Decoder(\n",
    "            n_tgt_vocab=n_tgt_vocab,\n",
    "            n_position=n_position,\n",
    "            d_word_vec=d_word_vec,\n",
    "            d_model=d_model,\n",
    "            d_inner=d_inner,\n",
    "            n_layers=n_layers,\n",
    "            n_head=n_head,\n",
    "            d_k=d_k,\n",
    "            d_v=d_v,\n",
    "            pad_idx=tgt_pad_idx,\n",
    "            dropout=dropout,\n",
    "            scale_emb=scale_emb\n",
    "        )\n",
    "\n",
    "        # 这是一个线性层，将解码器的输出映射为目标语言词汇表的 logits\n",
    "        self.tgt_word_prj = nn.Linear(d_model, n_tgt_vocab, bias=False)\n",
    "\n",
    "        # 使用 Xavier 初始化方法对所有权重矩阵进行初始化\n",
    "        for p in self.parameters():\n",
    "            if p.dim > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "        assert d_model == d_word_vec, \\\n",
    "        'To facilitate the residual connections, \\\n",
    "         the dimensions of all module outputs shall be the same.'\n",
    "\n",
    "        # 如果启用了 tgt_emb_prj_weight_sharing，则该层的权重会与目标语言嵌入矩阵共享\n",
    "        if tgt_emb_prj_weight_sharing:\n",
    "            # Share the weight between target word embedding & last dense layer\n",
    "            self.tgt_word_prj.weight = self.decoder.tgt_word_emb.weight\n",
    "\n",
    "        # 如果启用 emb_src_tgt_weight_sharing，则源语言和目标语言的嵌入矩阵权重会被共享。\n",
    "        if emb_src_tgt_weight_sharing:\n",
    "            self.encoder.src_word_emb.weight = self.decoder.tgt_word_emb.weight\n",
    "    \n",
    "    def forward(self, src_seq, tgt_seq):\n",
    "        src_mask = get_pad_mask(src_seq, self.src_pad_idx)\n",
    "        tgt_mask = get_pad_mask(tgt_seq, self.tgt_pad_idx) & get_subsequent_mask(tgt_seq)\n",
    "\n",
    "        enc_output, *_ = self.encoder(src_seq, src_mask)\n",
    "        dec_output, *_ = self.decoder(tgt_seq, tgt_mask, enc_output, src_mask)\n",
    "\n",
    "        # 将解码器输出映射为目标语言词汇表的 logits\n",
    "        seq_logit = self.tgt_word_prj(dec_output)\n",
    "\n",
    "        # 如果启用了投影缩放，则对 logits 进行缩放\n",
    "        if self.scale_prj:\n",
    "            seq_logit *= self.d_model ** -0.5\n",
    "\n",
    "        # 将 logits 展平为 [batch_size * seq_len, vocab_size] 的形状，便于计算损失函数\n",
    "        return seq_logit.view(-1, seq_logit.size(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题\n",
    "90%的都理解了，但是对于什么投影共享之类的，很理解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 优化模块\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ScheduledOptim():\n",
    "    '''A simple wrapper class for learning rate scheduling'''\n",
    "\n",
    "    def __init__(self, optimizer, lr_mul, d_model, n_warmup_steps):\n",
    "        self._optimizer = optimizer\n",
    "        self.lr_mul = lr_mul\n",
    "        self.d_model = d_model\n",
    "        self.n_warmup_steps = n_warm_steps\n",
    "        self.n_steps = 0\n",
    "    \n",
    "    def step_and_update_lr(self):\n",
    "        self._update_learning_rate()\n",
    "        self._optimizer.step()\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        \"Zero out the gradients with the inner optimizer\"\n",
    "        self._optimizer.zero_grad()\n",
    "    \n",
    "    def _get_lr_scale(self):\n",
    "        d_model = self.d_model\n",
    "        n_steps, n_warmup_steps = self.n_steps, self.n_warmup_steps\n",
    "        return (d_model ** -0.5) * min(n_steps ** (-0.5), n_steps * n_warmup_steps ** (-1.5))\n",
    "\n",
    "    def _update_learning_rate(self):\n",
    "        ''' Learning rate scheduling per step '''\n",
    "\n",
    "        self.n_steps += 1\n",
    "        lr = self.lr_mul * self._get_lr_scale()\n",
    "\n",
    "        for param_group in self._optimizer.param_groups:\n",
    "            param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练函数模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import math\n",
    "import time\n",
    "import dill as pickle\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "def call_loss(pred, gold, tgt_pad_idx, smoothing=False):\n",
    "    ''' Calculate cross entropy loss, apply label smoothing if needed. '''\n",
    "    \"\"\"\n",
    "    pred: [batch_size * seq_len, vocab_size]\n",
    "    gold：目标序列的真实标签，形状为 [batch_size, seq_len]\n",
    "    trg_pad_idx：目标语言中的填充符索引（PAD token），用于忽略填充部分的损失。\n",
    "\n",
    "    \"\"\"\n",
    "    # gold: 真实标签\n",
    "    gold = gold.contiguous().view(-1)\n",
    "    if smoothing:\n",
    "        eps = 0.1\n",
    "        n_class = pred.size(1)\n",
    "\n",
    "        one_hot = torch.zeros_like(pred).scatter(1, gold.view(-1,1), 1) #\n",
    "        one_hot = one_hot*(1-eps)+(1-one_hot)*eps / (n_class - 1)\n",
    "        log_prb = F.log_softmax(pred, dim=1) # 使用 F.log_softmax 计算预测结果的对数概率\n",
    "\n",
    "        non_pad_mask = gold.ne(tgt_pad_idx) #\n",
    "        loss = -(one_hot*log_prb).sum(dim=1)\n",
    "        loss = loss.masked_select(non_pad_mask).sum()\n",
    "    else:\n",
    "        loss = F.cross_entropy(pred, gold, ignore_index=tgt_pad_idx, reduction='sum')\n",
    "    return loss\n",
    "\n",
    "def call_performance(pred, gold, tgt_pad_idx, smoothing=False):\n",
    "    \"\"\"Apply label smoothing is needed\"\"\"\n",
    "\n",
    "    loss = call_loss(pred, gold, tgt_pad_idx, smoothing=smoothing)\n",
    "\n",
    "    pred = pred.max(1)[1]\n",
    "    gold = gold.contiguous().view(-1)\n",
    "    non_pad_mask = gold.ne(tgt_pad_idx)\n",
    "    n_correct = pred.eq(gold).masked_select(non_pad_mask).sum().item()\n",
    "    n_word = non_pad_mask.sum().item()\n",
    "    return loss, n_correct, n_word\n",
    "\n",
    "def patch_src(src, pad_idx):\n",
    "    src = src.transpose(0,1)\n",
    "    return src\n",
    "\n",
    "# 不是很理解这一点\n",
    "def patch_tgt(tgt, pad_idx):\n",
    "    \"\"\"\n",
    "    tgt：用于解码器的输入，不包含最后一个 token，形状为 [batch_size, seq_len - 1]。\n",
    "    gold：用于计算损失，包含从第二个 token 开始的目标序列，并展平为一维张量。形状为 [batch_size * (seq_len - 1)]\n",
    "    \"\"\"\n",
    "    tgt = tgt.transpose(0,1)\n",
    "    tgt, gold = tgt[:,:-1], tgt[:,:-1].contiguous().view(-1)\n",
    "    return tgt, gold\n",
    "\n",
    "def train_epoch(model, training_data, optimizer, opt, device, smoothing):\n",
    "    model.train()\n",
    "    total_loss, n_word_total, n_word_correct = 0, 0, 0\n",
    "    desc = '  - (Training)   '\n",
    "    for batch in tqdm(training_data, mininterval=2, desc=desc, leave=False):\n",
    "        # prepare data\n",
    "        src_seq = patch_src(batch.src, opt.src_pad_idx).to(device)\n",
    "        tgt_seq, gold = map(lambda x: x.to(device), patch_tgt(batch.tgt, opt.tgt_pad_idx))\n",
    "\n",
    "        # forward\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(src_seq, tgt_seq)\n",
    "\n",
    "        # backward and update parameters\n",
    "        loss, n_correct, n_word = call_performance(pred, gold, opt.tgt_pad_idx, smoothing=smoothing)\n",
    "        loss.backward()\n",
    "        optimizer.step_and_update_lr()\n",
    "\n",
    "        n_word_total += n_word\n",
    "        n_word_correct += n_correct\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    loss_per_word = total_loss / n_word_total\n",
    "    accuracy = n_word_correct / n_word_total\n",
    "    return loss_per_word, accuracy\n",
    "\n",
    "def eval_epoch(model, validation_data, device, opt):\n",
    "    model.eval()\n",
    "    total_loss, n_word_total, n_word_correct = 0,0,0\n",
    "    desc = '  - (Validation) '\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(validation_data, mininterval=2, desc=desc, leave=False):\n",
    "            src_seq = patch_src(batch.src, opt.src_pad_idx).to(device)\n",
    "            tgt_seq, gold = map(lambda x:x.to(device), patch_tgt(batch.tgt, opt.tgt_pad_idx))\n",
    "\n",
    "            pred = model(src_seq, trg_seq)\n",
    "            loss, n_correct, n_word = call_performance(pred, gold, opt.tgt_pad_idx, smoothing=False)\n",
    "\n",
    "            n_word_total += n_word\n",
    "            n_word_correct += n_correct\n",
    "            total_loss += loss.item()\n",
    "    loss_per_word = total_loss / n_word_total\n",
    "    accuracy = n_word_correct / n_word_total\n",
    "    return loss_per_word, accuracy\n",
    "\n",
    "def train(model, training_data, validation_data, optimizer, device, opt):\n",
    "    # Use tensorboard to plot curves, e.g. perplexity, accuracy, learning rate\n",
    "    if opt.use_tb:\n",
    "        print(\"[Info] Use Tensorboard\")\n",
    "        from torch.utils.tensorbard import SummaryWriter\n",
    "        tb_writer = SummaryWriter(log_dir=opt.output_dir)\n",
    "    log_train_file = opt.train_log_dir\n",
    "    log_valid_file = opt.valid_log_dir\n",
    "\n",
    "    print('[Info] Training performance will be written to file: {} and {}'.format(log_train_file, log_valid_file))\n",
    "\n",
    "    with open(log_train_file, 'w') as log_tf, open(log_valid_file, 'w') as log_vf:\n",
    "        log_tf.write('epoch,loss,ppl,accuracy\\n')\n",
    "        log_vf.write('epoch,loss,ppl,accuracy\\n')\n",
    "    \n",
    "    def print_performance(header, ppl, accu, start_time, lr):\n",
    "        print('  - {header:12} ppl: {ppl: 8.5f}, accuracy: {accu:3.3f} %, lr: {lr:8.5f}, '\\\n",
    "              'elapse: {elapse:3.3f} min'.format(\n",
    "                  header=f\"({header})\", ppl=ppl,\n",
    "                  accu=100*accu, elapse=(time.time()-start_time)/60, lr=lr))\n",
    "    \n",
    "    valid_loss = []\n",
    "    for epoch_i in range(opt.epoch):\n",
    "        print('[ Epoch', epoch_i, ']')\n",
    "\n",
    "        start = time.time()\n",
    "        train_loss, train_accu = train_epoch(\n",
    "            model,\n",
    "            training_data,\n",
    "            optimizer,\n",
    "            opt,\n",
    "            device,\n",
    "            smoothing=opt.label_smoothing\n",
    "        )\n",
    "        train_ppl = math.exp(min(train_loss, 100))\n",
    "        lr = optimizer._optimizer.param_groups[0]['lr']\n",
    "        print_performances('Training', train_ppl, train_accu, start, lr)\n",
    "\n",
    "        start = time.time()\n",
    "        valid_loss, valid_accu = eval_epoch(\n",
    "            model, \n",
    "            validation_data, \n",
    "            device, \n",
    "            opt)\n",
    "        valid_ppl = math.exp(min(valid_loss, 100))\n",
    "        print_performances('Validation', valid_ppl, valid_accu, start, lr)\n",
    "        valid_losses += [valid_loss]\n",
    "        checkpoint = {'epoch': epoch_i, 'settings': opt, 'model': model.state_dict()}\n",
    "\n",
    "        if opt.save_mode == 'all':\n",
    "            model_name = 'model_accu_{accu:3.3f}.chkpt'.format(accu=100*valid_accu)\n",
    "            torch.save(checkpoint, model_name)\n",
    "        elif opt.save_mode == 'best':\n",
    "            model_name = 'model.chkpt'\n",
    "            if valid_loss <= min(valid_losses):\n",
    "                torch.save(checkpoint, os.path.join(opt.output_dir, model_name))\n",
    "                print('    - [Info] The checkpoint file has been updated.')\n",
    "\n",
    "        with open(log_train_file, 'a') as log_tf, open(log_valid_file, 'a') as log_vf:\n",
    "            log_tf.write('{epoch},{loss: 8.5f},{ppl: 8.5f},{accu:3.3f}\\n'.format(\n",
    "                epoch=epoch_i, loss=train_loss,\n",
    "                ppl=train_ppl, accu=100*train_accu))\n",
    "            log_vf.write('{epoch},{loss: 8.5f},{ppl: 8.5f},{accu:3.3f}\\n'.format(\n",
    "                epoch=epoch_i, loss=valid_loss,\n",
    "                ppl=valid_ppl, accu=100*valid_accu))\n",
    "\n",
    "        if opt.use_tb:\n",
    "            tb_writer.add_scalars('ppl', {'train': train_ppl, 'val': valid_ppl}, epoch_i)\n",
    "            tb_writer.add_scalars('accuracy', {'train': train_accu*100, 'val': valid_accu*100}, epoch_i)\n",
    "            tb_writer.add_scalar('learning_rate', lr, epoch_i)\n",
    "\n",
    "def main():\n",
    "    ''' \n",
    "    Usage:\n",
    "    python train.py -data_pkl m30k_deen_shr.pkl -log m30k_deen_shr -embs_share_weight -proj_share_weight -label_smoothing -output_dir output -b 256 -warmup 128000\n",
    "    '''\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument('-data_pkl', default=None)     # all-in-1 data pickle or bpe field\n",
    "\n",
    "    parser.add_argument('-train_path', default=None)   # bpe encoded data\n",
    "    parser.add_argument('-val_path', default=None)     # bpe encoded data\n",
    "\n",
    "    parser.add_argument('-epoch', type=int, default=10)\n",
    "    parser.add_argument('-b', '--batch_size', type=int, default=2048)\n",
    "\n",
    "    parser.add_argument('-d_model', type=int, default=512)\n",
    "    parser.add_argument('-d_inner_hid', type=int, default=2048)\n",
    "    parser.add_argument('-d_k', type=int, default=64)\n",
    "    parser.add_argument('-d_v', type=int, default=64)\n",
    "\n",
    "    parser.add_argument('-n_head', type=int, default=8)\n",
    "    parser.add_argument('-n_layers', type=int, default=6)\n",
    "    parser.add_argument('-warmup','--n_warmup_steps', type=int, default=4000)\n",
    "    parser.add_argument('-lr_mul', type=float, default=2.0)\n",
    "    parser.add_argument('-seed', type=int, default=None)\n",
    "\n",
    "    parser.add_argument('-dropout', type=float, default=0.1)\n",
    "    parser.add_argument('-embs_share_weight', action='store_true')\n",
    "    parser.add_argument('-proj_share_weight', action='store_true')\n",
    "    parser.add_argument('-scale_emb_or_prj', type=str, default='prj')\n",
    "\n",
    "    parser.add_argument('-output_dir', type=str, default=None)\n",
    "    parser.add_argument('-use_tb', action='store_true')\n",
    "    parser.add_argument('-save_mode', type=str, choices=['all', 'best'], default='best')\n",
    "\n",
    "    parser.add_argument('-no_cuda', action='store_true')\n",
    "    parser.add_argument('-label_smoothing', action='store_true')\n",
    "\n",
    "    opt = parser.parse_args()\n",
    "    opt.cuda = not opt.no_cuda\n",
    "    opt.d_word_vec = opt.d_model\n",
    "\n",
    "    # https://pytorch.org/docs/stable/notes/randomness.html\n",
    "    # For reproducibility\n",
    "    if opt.seed is not None:\n",
    "        torch.manual_seed(opt.seed)\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        # torch.set_deterministic(True)\n",
    "        np.random.seed(opt.seed)\n",
    "        random.seed(opt.seed)\n",
    "\n",
    "    if not opt.output_dir:\n",
    "        print('No experiment result will be saved.')\n",
    "        raise\n",
    "\n",
    "    if not os.path.exists(opt.output_dir):\n",
    "        os.makedirs(opt.output_dir)\n",
    "\n",
    "    if opt.batch_size < 2048 and opt.n_warmup_steps <= 4000:\n",
    "        print('[Warning] The warmup steps may be not enough.\\n'\\\n",
    "              '(sz_b, warmup) = (2048, 4000) is the official setting.\\n'\\\n",
    "              'Using smaller batch w/o longer warmup may cause '\\\n",
    "              'the warmup stage ends with only little data trained.')\n",
    "\n",
    "    device = torch.device('cuda' if opt.cuda else 'cpu')\n",
    "\n",
    "    #========= Loading Dataset =========#\n",
    "\n",
    "    if all((opt.train_path, opt.val_path)):\n",
    "        training_data, validation_data = prepare_dataloaders_from_bpe_files(opt, device)\n",
    "    elif opt.data_pkl:\n",
    "        training_data, validation_data = prepare_dataloaders(opt, device)\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "    print(opt)\n",
    "\n",
    "    transformer = Transformer(\n",
    "        opt.src_vocab_size,\n",
    "        opt.trg_vocab_size,\n",
    "        src_pad_idx=opt.src_pad_idx,\n",
    "        trg_pad_idx=opt.trg_pad_idx,\n",
    "        trg_emb_prj_weight_sharing=opt.proj_share_weight,\n",
    "        emb_src_trg_weight_sharing=opt.embs_share_weight,\n",
    "        d_k=opt.d_k,\n",
    "        d_v=opt.d_v,\n",
    "        d_model=opt.d_model,\n",
    "        d_word_vec=opt.d_word_vec,\n",
    "        d_inner=opt.d_inner_hid,\n",
    "        n_layers=opt.n_layers,\n",
    "        n_head=opt.n_head,\n",
    "        dropout=opt.dropout,\n",
    "        scale_emb_or_prj=opt.scale_emb_or_prj).to(device)\n",
    "\n",
    "    optimizer = ScheduledOptim(\n",
    "        optim.Adam(transformer.parameters(), betas=(0.9, 0.98), eps=1e-09),\n",
    "        opt.lr_mul, opt.d_model, opt.n_warmup_steps)\n",
    "\n",
    "    train(transformer, training_data, validation_data, optimizer, device, opt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.4631), 2, 2)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = torch.tensor([[0.9, 0.1, 0.0], [0.1, 0.8, 0.1], [0.2, 0.2, 0.6]])\n",
    "gold = torch.tensor([0, 1, 2])\n",
    "tgt_pad_idx = 2\n",
    "smoothing = True\n",
    "\n",
    "call_performance(pred, gold, tgt_pad_idx, smoothing)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
