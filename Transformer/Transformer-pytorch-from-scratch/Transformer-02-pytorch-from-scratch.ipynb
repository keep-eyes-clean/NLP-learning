{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eng-Dem 语言数据预处理操作 \n",
    "## 基础流程\n",
    "原始语料 → 清洗过滤 → 分词训练 → 数据加载 → 模型训练 → 评估优化 → 部署上线\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考他人的数据分析流程\n",
    "### 分析问题\n",
    "1. 自然语言的预处理流程我不熟悉，使用的第三方库我不熟悉\n",
    "2. torchtext的版本最高是0.18.0，匹配torch的2.3.0，但是torch的2.3.0与我的cuda版本12.2不匹配，不知道安装过程难度是否过大\n",
    "\n",
    "### 解决方法\n",
    "我之所以遇到这种问题，原因是因为torchtext已经停止维护，目前应该使用hugging face下的transformers库，进行数据预处理\n",
    "\n",
    "数据预处理刘彻个：\n",
    "- 1. 数据准备：留下本地数据的地址\n",
    "- 2. 加载 pretrained tokenizer，也可以本地训练然后加载 tokenizer\n",
    "    - 如何使用本地语料库训练tokenizer:\n",
    "        - 1. 加载语料库\n",
    "        - 2. 定义tokenizer参数并保存tokenizer\n",
    "        - 3. 基于语料库训练tokenizer\n",
    "        - 4. 加载测试 customed tokenizer\n",
    "        - 5. \n",
    "- 3. 定义数据读取与预训练函数\n",
    "- 4. 数据预处理与加载：创建dataset\n",
    "- 5. 创建Dataloader批量加载数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration epoch 1:\n",
    "使用预训练tokenizer进行数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. data path\n",
    "source_file = \"/harddisk1/SZC-Project/NLP-learning/Transformer/Transformer-pytorch-from-scratch/europarl-v7.de-en.en\"\n",
    "target_file = \"/harddisk1/SZC-Project/NLP-learning/Transformer/Transformer-pytorch-from-scratch/europarl-v7.de-en.de\"\n",
    "\n",
    "# 2. loading the language dataset\n",
    "def load_corpus(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = [line.strip() for line in f if line.strip()]\n",
    "    return lines\n",
    "\n",
    "en_corpus = load_corpus(source_file)\n",
    "de_corpus = load_corpus(target_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1911843"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(en_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "分词器已保存到 /harddisk1/SZC-Project/NLP-learning/Transformer/Transformer-pytorch-from-scratch/en_tokenizer.json\n",
      "\n",
      "\n",
      "\n",
      "分词器已保存到 /harddisk1/SZC-Project/NLP-learning/Transformer/Transformer-pytorch-from-scratch/de_tokenizer.json\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "def train_tokenizer(corpus, vocab_size, output_path, language_name):\n",
    "    \"\"\"训练自定义分词器\"\"\"\n",
    "    # 初始化分词器模型\n",
    "    tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "\n",
    "    # 预训练分词器（例如按照空格分割）\n",
    "    # ？？\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "    # 定义训练参数\n",
    "    trainer = BpeTrainer(\n",
    "        special_tokens = [\"[PAD]\",\"[UNK]\",\"[BOS]\",\"[EOS]\"],\n",
    "        vocab_size = vocab_size\n",
    "    )\n",
    "\n",
    "    # 训练分词器\n",
    "    tokenizer.train_from_iterator(corpus, trainer=trainer)\n",
    "\n",
    "    # 保存分词器\n",
    "    tokenizer.save(f\"{output_path}/{language_name}_tokenizer.json\")\n",
    "    print(f\"分词器已保存到 {output_path}/{language_name}_tokenizer.json\")\n",
    "\n",
    "train_tokenizer(en_corpus, vocab_size=30000, output_path=\"/harddisk1/SZC-Project/NLP-learning/Transformer/Transformer-pytorch-from-scratch\", language_name='en')\n",
    "train_tokenizer(de_corpus, vocab_size=30000, output_path=\"/harddisk1/SZC-Project/NLP-learning/Transformer/Transformer-pytorch-from-scratch\", language_name='de')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Tokenization:\n",
      "['this', 'is', 'my', 'community', '.', 'we', 'get', 'to', 'open', 'people', \"'\", 's', 'mind', 'to', 'accept', 'new', 'idea']\n",
      "German Tokenization:\n",
      "['Dies', 'ist', 'ein', 'Test', 'Satz', '.']\n"
     ]
    }
   ],
   "source": [
    "en_tokenizer_path = \"/harddisk1/SZC-Project/NLP-learning/Transformer/Transformer-pytorch-from-scratch/en_tokenizer.json\"\n",
    "de_tokenizer_path = \"/harddisk1/SZC-Project/NLP-learning/Transformer/Transformer-pytorch-from-scratch/de_tokenizer.json\"\n",
    "\n",
    "en_tokenizer = Tokenizer.from_file(en_tokenizer_path)\n",
    "de_tokenizer = Tokenizer.from_file(de_tokenizer_path)\n",
    "\n",
    "# 测试分词器\n",
    "test_sentence_en = \"this is my community. we get to open people's mind to accept new idea\"\n",
    "test_sentence_de = \"Dies ist ein Test Satz.\"\n",
    "\n",
    "print(\"English Tokenization:\")\n",
    "print(en_tokenizer.encode(test_sentence_en).tokens)\n",
    "\n",
    "print(\"German Tokenization:\")\n",
    "print(de_tokenizer.encode(test_sentence_de).tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将自定义分词器转换为Transformer模型\n",
    "\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "# create English tokenizer\n",
    "en_transformer_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object = en_tokenizer,\n",
    "    bos_token = \"[BOS]\",\n",
    "    eos_token = \"[EOS]\",\n",
    "    pad_token = \"[PAD]\",\n",
    "    unk_token = \"[UNK]\"\n",
    ")\n",
    "\n",
    "# create German tokenizer\n",
    "de_transformer_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object = de_tokenizer,\n",
    "    bos_token = \"[BOS]\",\n",
    "    eos_token = \"[EOS]\",\n",
    "    pad_token = \"[PAD]\",\n",
    "    unk_token = \"[UNK]\"   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 370,  317,  616, 2309,   17,  350,  673,  322, 1162,  640,   10,   85,\n",
      "         1409,  322, 1154,  605, 1954]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "{'input_ids': tensor([[1005,  371,  336, 6719, 7111,   17]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# 编码英语句子\n",
    "encoded_en = en_transformer_tokenizer(test_sentence_en, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "\n",
    "# 编码德语句子\n",
    "encoded_de = de_transformer_tokenizer(test_sentence_de, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "\n",
    "print(encoded_en)\n",
    "print(encoded_de)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([17])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_en['input_ids'].squeeze(0).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration epoch 2: \n",
    "将上述的自定义的tokenizer用于创建数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "import torch \n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "source_file = \"/harddisk1/SZC-Project/NLP-learning/Transformer/Transformer-pytorch-from-scratch/europarl-v7.de-en.en\"\n",
    "target_file = \"/harddisk1/SZC-Project/NLP-learning/Transformer/Transformer-pytorch-from-scratch/europarl-v7.de-en.de\"\n",
    "\n",
    "en_tokenizer_path = \"/harddisk1/SZC-Project/NLP-learning/Transformer/Transformer-pytorch-from-scratch/en_tokenizer.json\"\n",
    "de_tokenizer_path = \"/harddisk1/SZC-Project/NLP-learning/Transformer/Transformer-pytorch-from-scratch/de_tokenizer.json\"\n",
    "\n",
    "en_tokenizer = Tokenizer.from_file(en_tokenizer_path)\n",
    "de_tokenizer = Tokenizer.from_file(de_tokenizer_path)\n",
    "\n",
    "# create English tokenizer\n",
    "en_transformer_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object = en_tokenizer,\n",
    "    bos_token = \"[BOS]\",\n",
    "    eos_token = \"[EOS]\",\n",
    "    pad_token = \"[PAD]\",\n",
    "    unk_token = \"[UNK]\"\n",
    ")\n",
    "\n",
    "# create German tokenizer\n",
    "de_transformer_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object = de_tokenizer,\n",
    "    bos_token = \"[BOS]\",\n",
    "    eos_token = \"[EOS]\",\n",
    "    pad_token = \"[PAD]\",\n",
    "    unk_token = \"[UNK]\"  \n",
    "    )\n",
    "\n",
    "def load_corpus(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = [line.strip() for line in f if line.strip()]\n",
    "    return lines\n",
    "\n",
    "def train_tokenizer(corpus, vocab_size, output_path, language_name):\n",
    "    \"\"\"训练自定义分词器\"\"\"\n",
    "    # 初始化分词器模型\n",
    "    tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "\n",
    "    # 预训练分词器（例如按照空格分割）\n",
    "    # ？？\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "    # 定义训练参数\n",
    "    trainer = BpeTrainer(\n",
    "        special_tokens = [\"[PAD]\",\"[UNK]\",\"[BOS]\",\"[EOS]\"],\n",
    "        vocab_size = vocab_size\n",
    "    )\n",
    "\n",
    "    # 训练分词器\n",
    "    tokenizer.train_from_iterator(corpus, trainer=trainer)\n",
    "\n",
    "    # 保存分词器\n",
    "    tokenizer.save(f\"{output_path}/{language_name}_tokenizer.json\")\n",
    "    print(f\"分词器已保存到 {output_path}/{language_name}_tokenizer.json\")\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"自定义 collate_fn 用于处理不同长度的序列\"\"\"\n",
    "    input_ids = torch.stack([item['input_ids'] for item in batch])  # 使用列表推导式\n",
    "    attention_mask = torch.stack([item['attention_mask'] for item in batch])  # 使用列表推导式\n",
    "    labels = torch.stack([item['labels'] for item in batch])  # 使用列表推导式\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, src_lines, tgt_lines, src_transformer_tokenizer, tgt_transformer_tokenizer, max_length=128):\n",
    "        self.src_lines = src_lines\n",
    "        self.tgt_lines = tgt_lines\n",
    "        self.src_transformer_tokenizer = src_transformer_tokenizer\n",
    "        self.tgt_transformer_tokenizer = tgt_transformer_tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.src_lines)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 对源语言编码\n",
    "        src_encoding = self.src_transformer_tokenizer(\n",
    "            self.src_lines[idx],\n",
    "            padding = 'max_length',\n",
    "            truncation = True,\n",
    "            max_length = self.max_length,\n",
    "            return_tensors = \"pt\"\n",
    "        )\n",
    "\n",
    "        tgt_encoding = self.tgt_transformer_tokenizer(\n",
    "            self.tgt_lines[idx],\n",
    "            padding = 'max_length',\n",
    "            truncation = True,\n",
    "            max_length = self.max_length,\n",
    "            return_tensors = \"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": src_encoding['input_ids'].squeeze(0),\n",
    "            \"attention_mask\": src_encoding['attention_mask'].squeeze(0),\n",
    "            'labels': tgt_encoding['input_ids'].squeeze(0)\n",
    "        }\n",
    "\n",
    "src_lines = load_corpus(source_file)\n",
    "tgt_lines = load_corpus(target_file)\n",
    "\n",
    "train_dataset = TranslationDataset(src_lines, tgt_lines, en_transformer_tokenizer, de_transformer_tokenizer, max_length=128)\n",
    "\n",
    "sample_ratio = 0.1  # 采样10%数据\n",
    "indices = np.random.choice(\n",
    "    len(train_dataset), \n",
    "    int(len(train_dataset)*sample_ratio),\n",
    "    replace=False\n",
    ")\n",
    "sampler = SubsetRandomSampler(indices)\n",
    "\n",
    "sampled_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=64,\n",
    "    sampler=sampler,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word:\n",
      "tensor([[  584,   928,  1162,  ...,     0,     0,     0],\n",
      "        [  403,  8061,   323,  ...,     0,     0,     0],\n",
      "        [10529,    15,   310,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  480,   360,  1426,  ...,     0,     0,     0],\n",
      "        [  403,   543,   864,  ...,     0,     0,     0],\n",
      "        [  437,   475,    15,  ...,     0,     0,     0]])\n",
      "torch.Size([64, 128])\n",
      "\n",
      "attention mask:\n",
      "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])\n",
      "torch.Size([64, 128])\n",
      "labels:\n",
      "tensor([[ 1101,  1524,   371,  ...,     0,     0,     0],\n",
      "        [12620,  4379,  2744,  ...,     0,     0,     0],\n",
      "        [ 5056,    17,    16,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [ 3046,  1060,   379,  ...,     0,     0,     0],\n",
      "        [ 3046,   416,   366,  ...,     0,     0,     0],\n",
      "        [ 1748,   845,  2959,  ...,     0,     0,     0]])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([63, 128])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    print(\"word:\")\n",
    "    print(batch['input_ids'])\n",
    "    print(batch['input_ids'].size())\n",
    "    print(\"\")\n",
    "    print(\"attention mask:\")\n",
    "    print(batch['attention_mask'])\n",
    "    print(batch['attention_mask'].size())\n",
    "    print(\"labels:\")\n",
    "    print(batch['labels'])\n",
    "    print(batch['labels'].size())\n",
    "    print(batch['labels'][1:].size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration epoch 3: \n",
    "创建 Transformer类，先尝试使用pytorch的方式创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=512, n_head=8, num_encoder_layers=6, num_decoder_layers=6, dim_feedforward=2048, dropout=0.1, max_len=128):\n",
    "        super(TransformerModel,self).__init__()\n",
    "\n",
    "        # 位置编码\n",
    "        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(max_len, 1, d_model), requires_grad=True)\n",
    "\n",
    "        # Transformer\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model = d_model,\n",
    "            nhead = n_head,\n",
    "            num_encoder_layers = num_encoder_layers,\n",
    "            num_decoder_layers = num_decoder_layers,\n",
    "            dim_feedforward = dim_feedforward,\n",
    "            dropout = dropout\n",
    "        )\n",
    "\n",
    "        # 输出层\n",
    "        self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        \"\"\"生成掩码，防止解码器看到未来的时间步\"\"\"\n",
    "        return self.transformer.generate_square_subsequent_mask(sz)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
    "        src_emb = self.src_embedding(src) + self.positional_encoding[:src.size(0)]\n",
    "        tgt_emb = self.tgt_embedding(tgt) + self.positional_encoding[:tgt.size(0)]\n",
    "\n",
    "        # Transformer前向传播\n",
    "        output = self.transformer(src_emb, tgt_emb, src_mask=src_mask, tgt_mask=tgt_mask)\n",
    "\n",
    "        # 线性输出层\n",
    "        output = self.fc_out(output)\n",
    "        return output\n",
    "\n",
    "# Training function\n",
    "def train(model, dataloader, optimizer, criterion, device, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    i = 0\n",
    "    for batch in dataloader:\n",
    "        if i % 10 == 0:\n",
    "            print(f\"batch:{i}\")\n",
    "        input_ids = batch['input_ids']\n",
    "        input_ids = input_ids.to(device)\n",
    "\n",
    "        attention_mask = batch['attention_mask']\n",
    "        attention_mask = attention_mask.to(device)\n",
    "\n",
    "        labels = batch['labels']\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # 创建mask\n",
    "        src_mask = model.generate_square_subsequent_mask(input_ids.size(0)).to(device)\n",
    "        tgt_mask = model.generate_square_subsequent_mask(labels.size(0) - 1).to(device)\n",
    "\n",
    "        # 前向传播\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input_ids, labels[:-1], src_mask=src_mask, tgt_mask=tgt_mask)\n",
    "\n",
    "        # 计算损失\n",
    "        loss = criterion(output.view(-1, output.size(-1)), labels[1:].reshape(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        # 更新参数\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        i += 1\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Evaluate function\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            src_mask = model.generate_square_subsequent_mask(input_ids.size(0)).to(device)\n",
    "            tgt_mask = model.generate_square_subsequent_mask(labels.size(0)).to(device)\n",
    "\n",
    "            output = model(input_ids, labels[:-1], src_mask=src_mask, tgt_mask=tgt_mask)\n",
    "            loss = criterion(output.view(-1, output.size(-1)), labels[1:].reshape(-1))\n",
    "            total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Evaluation Loss: {avg_loss:.4f}\")\n",
    "    return avf_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/songzc/anaconda3/envs/py10/lib/python3.10/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch:0\n",
      "batch:10\n",
      "batch:20\n",
      "batch:30\n",
      "batch:40\n",
      "batch:50\n",
      "batch:60\n",
      "batch:70\n",
      "batch:80\n",
      "batch:90\n",
      "batch:100\n",
      "batch:110\n",
      "batch:120\n",
      "batch:130\n",
      "batch:140\n",
      "batch:150\n",
      "batch:160\n",
      "batch:170\n",
      "batch:180\n",
      "batch:190\n",
      "batch:200\n",
      "batch:210\n",
      "batch:220\n",
      "batch:230\n",
      "batch:240\n",
      "batch:250\n",
      "batch:260\n",
      "batch:270\n",
      "batch:280\n",
      "batch:290\n",
      "batch:300\n",
      "batch:310\n",
      "batch:320\n",
      "batch:330\n",
      "batch:340\n",
      "batch:350\n",
      "batch:360\n",
      "batch:370\n",
      "batch:380\n",
      "batch:390\n",
      "batch:400\n",
      "batch:410\n",
      "batch:420\n",
      "batch:430\n",
      "batch:440\n",
      "batch:450\n",
      "batch:460\n",
      "batch:470\n",
      "batch:480\n",
      "batch:490\n",
      "batch:500\n",
      "batch:510\n",
      "batch:520\n",
      "batch:530\n",
      "batch:540\n",
      "batch:550\n",
      "batch:560\n",
      "batch:570\n",
      "batch:580\n",
      "batch:590\n",
      "batch:600\n",
      "batch:610\n",
      "batch:620\n",
      "batch:630\n",
      "batch:640\n",
      "batch:650\n",
      "batch:660\n",
      "batch:670\n",
      "batch:680\n",
      "batch:690\n",
      "batch:700\n",
      "batch:710\n",
      "batch:720\n",
      "batch:730\n",
      "batch:740\n",
      "batch:750\n",
      "batch:760\n",
      "batch:770\n",
      "batch:780\n",
      "batch:790\n",
      "batch:800\n",
      "batch:810\n",
      "batch:820\n",
      "batch:830\n",
      "batch:840\n",
      "batch:850\n",
      "batch:860\n",
      "batch:870\n",
      "batch:880\n",
      "batch:890\n",
      "batch:900\n",
      "batch:910\n",
      "batch:920\n",
      "batch:930\n",
      "batch:940\n",
      "batch:950\n",
      "batch:960\n",
      "batch:970\n",
      "batch:980\n",
      "batch:990\n",
      "batch:1000\n",
      "batch:1010\n",
      "batch:1020\n",
      "batch:1030\n",
      "batch:1040\n",
      "batch:1050\n",
      "batch:1060\n",
      "batch:1070\n",
      "batch:1080\n",
      "batch:1090\n",
      "batch:1100\n",
      "batch:1110\n",
      "batch:1120\n",
      "batch:1130\n",
      "batch:1140\n",
      "batch:1150\n",
      "batch:1160\n",
      "batch:1170\n",
      "batch:1180\n",
      "batch:1190\n",
      "batch:1200\n",
      "batch:1210\n",
      "batch:1220\n",
      "batch:1230\n",
      "batch:1240\n",
      "batch:1250\n",
      "batch:1260\n",
      "batch:1270\n",
      "batch:1280\n",
      "batch:1290\n",
      "batch:1300\n",
      "batch:1310\n",
      "batch:1320\n",
      "batch:1330\n",
      "batch:1340\n",
      "batch:1350\n",
      "batch:1360\n",
      "batch:1370\n",
      "batch:1380\n",
      "batch:1390\n",
      "batch:1400\n",
      "batch:1410\n",
      "batch:1420\n",
      "batch:1430\n",
      "batch:1440\n",
      "batch:1450\n",
      "batch:1460\n",
      "batch:1470\n",
      "batch:1480\n",
      "batch:1490\n",
      "batch:1500\n",
      "batch:1510\n",
      "batch:1520\n",
      "batch:1530\n",
      "batch:1540\n",
      "batch:1550\n",
      "batch:1560\n",
      "batch:1570\n",
      "batch:1580\n",
      "batch:1590\n",
      "batch:1600\n",
      "batch:1610\n",
      "batch:1620\n",
      "batch:1630\n",
      "batch:1640\n",
      "batch:1650\n",
      "batch:1660\n",
      "batch:1670\n",
      "batch:1680\n",
      "batch:1690\n",
      "batch:1700\n",
      "batch:1710\n",
      "batch:1720\n",
      "batch:1730\n",
      "batch:1740\n",
      "batch:1750\n",
      "batch:1760\n",
      "batch:1770\n",
      "batch:1780\n",
      "batch:1790\n",
      "batch:1800\n",
      "batch:1810\n",
      "batch:1820\n",
      "batch:1830\n",
      "batch:1840\n",
      "batch:1850\n",
      "batch:1860\n",
      "batch:1870\n",
      "batch:1880\n",
      "batch:1890\n",
      "batch:1900\n",
      "batch:1910\n",
      "batch:1920\n",
      "batch:1930\n",
      "batch:1940\n",
      "batch:1950\n",
      "batch:1960\n",
      "batch:1970\n",
      "batch:1980\n",
      "batch:1990\n",
      "batch:2000\n",
      "batch:2010\n",
      "batch:2020\n",
      "batch:2030\n",
      "batch:2040\n",
      "batch:2050\n",
      "batch:2060\n",
      "batch:2070\n",
      "batch:2080\n",
      "batch:2090\n",
      "batch:2100\n",
      "batch:2110\n",
      "batch:2120\n",
      "batch:2130\n",
      "batch:2140\n",
      "batch:2150\n",
      "batch:2160\n",
      "batch:2170\n",
      "batch:2180\n",
      "batch:2190\n",
      "batch:2200\n",
      "batch:2210\n",
      "batch:2220\n",
      "batch:2230\n",
      "batch:2240\n",
      "batch:2250\n",
      "batch:2260\n",
      "batch:2270\n",
      "batch:2280\n",
      "batch:2290\n",
      "batch:2300\n",
      "batch:2310\n",
      "batch:2320\n",
      "batch:2330\n",
      "batch:2340\n",
      "batch:2350\n",
      "batch:2360\n",
      "batch:2370\n",
      "batch:2380\n",
      "batch:2390\n",
      "batch:2400\n",
      "batch:2410\n",
      "batch:2420\n",
      "batch:2430\n",
      "batch:2440\n",
      "batch:2450\n",
      "batch:2460\n",
      "batch:2470\n",
      "batch:2480\n",
      "batch:2490\n",
      "batch:2500\n",
      "batch:2510\n",
      "batch:2520\n",
      "batch:2530\n",
      "batch:2540\n",
      "batch:2550\n",
      "batch:2560\n",
      "batch:2570\n",
      "batch:2580\n",
      "batch:2590\n",
      "batch:2600\n",
      "batch:2610\n",
      "batch:2620\n",
      "batch:2630\n",
      "batch:2640\n",
      "batch:2650\n",
      "batch:2660\n",
      "batch:2670\n",
      "batch:2680\n",
      "batch:2690\n",
      "batch:2700\n",
      "batch:2710\n",
      "batch:2720\n",
      "batch:2730\n",
      "batch:2740\n",
      "batch:2750\n",
      "batch:2760\n",
      "batch:2770\n",
      "batch:2780\n",
      "batch:2790\n",
      "batch:2800\n",
      "batch:2810\n",
      "batch:2820\n",
      "batch:2830\n",
      "batch:2840\n",
      "batch:2850\n",
      "batch:2860\n",
      "batch:2870\n",
      "batch:2880\n",
      "batch:2890\n",
      "batch:2900\n",
      "batch:2910\n",
      "batch:2920\n",
      "batch:2930\n",
      "batch:2940\n",
      "batch:2950\n",
      "batch:2960\n",
      "batch:2970\n",
      "batch:2980\n",
      "Epoch 0, Loss: 7.0288\n",
      "batch:0\n",
      "batch:10\n",
      "batch:20\n",
      "batch:30\n",
      "batch:40\n",
      "batch:50\n",
      "batch:60\n",
      "batch:70\n",
      "batch:80\n",
      "batch:90\n",
      "batch:100\n",
      "batch:110\n",
      "batch:120\n",
      "batch:130\n",
      "batch:140\n",
      "batch:150\n",
      "batch:160\n",
      "batch:170\n",
      "batch:180\n",
      "batch:190\n",
      "batch:200\n",
      "batch:210\n",
      "batch:220\n",
      "batch:230\n",
      "batch:240\n",
      "batch:250\n",
      "batch:260\n",
      "batch:270\n",
      "batch:280\n",
      "batch:290\n",
      "batch:300\n",
      "batch:310\n",
      "batch:320\n",
      "batch:330\n",
      "batch:340\n",
      "batch:350\n",
      "batch:360\n",
      "batch:370\n",
      "batch:380\n",
      "batch:390\n",
      "batch:400\n",
      "batch:410\n",
      "batch:420\n",
      "batch:430\n",
      "batch:440\n",
      "batch:450\n",
      "batch:460\n",
      "batch:470\n",
      "batch:480\n",
      "batch:490\n",
      "batch:500\n",
      "batch:510\n",
      "batch:520\n",
      "batch:530\n",
      "batch:540\n",
      "batch:550\n",
      "batch:560\n",
      "batch:570\n",
      "batch:580\n",
      "batch:590\n",
      "batch:600\n",
      "batch:610\n",
      "batch:620\n",
      "batch:630\n",
      "batch:640\n",
      "batch:650\n",
      "batch:660\n",
      "batch:670\n",
      "batch:680\n",
      "batch:690\n",
      "batch:700\n",
      "batch:710\n",
      "batch:720\n",
      "batch:730\n",
      "batch:740\n",
      "batch:750\n",
      "batch:760\n",
      "batch:770\n",
      "batch:780\n",
      "batch:790\n",
      "batch:800\n",
      "batch:810\n",
      "batch:820\n",
      "batch:830\n",
      "batch:840\n",
      "batch:850\n",
      "batch:860\n",
      "batch:870\n",
      "batch:880\n",
      "batch:890\n",
      "batch:900\n",
      "batch:910\n",
      "batch:920\n",
      "batch:930\n",
      "batch:940\n",
      "batch:950\n",
      "batch:960\n",
      "batch:970\n",
      "batch:980\n",
      "batch:990\n",
      "batch:1000\n",
      "batch:1010\n",
      "batch:1020\n",
      "batch:1030\n",
      "batch:1040\n",
      "batch:1050\n",
      "batch:1060\n",
      "batch:1070\n",
      "batch:1080\n",
      "batch:1090\n",
      "batch:1100\n",
      "batch:1110\n",
      "batch:1120\n",
      "batch:1130\n",
      "batch:1140\n",
      "batch:1150\n",
      "batch:1160\n",
      "batch:1170\n",
      "batch:1180\n",
      "batch:1190\n",
      "batch:1200\n",
      "batch:1210\n",
      "batch:1220\n",
      "batch:1230\n",
      "batch:1240\n",
      "batch:1250\n",
      "batch:1260\n",
      "batch:1270\n",
      "batch:1280\n",
      "batch:1290\n",
      "batch:1300\n",
      "batch:1310\n",
      "batch:1320\n",
      "batch:1330\n",
      "batch:1340\n",
      "batch:1350\n",
      "batch:1360\n",
      "batch:1370\n",
      "batch:1380\n",
      "batch:1390\n",
      "batch:1400\n",
      "batch:1410\n",
      "batch:1420\n",
      "batch:1430\n",
      "batch:1440\n",
      "batch:1450\n",
      "batch:1460\n",
      "batch:1470\n",
      "batch:1480\n",
      "batch:1490\n",
      "batch:1500\n",
      "batch:1510\n",
      "batch:1520\n",
      "batch:1530\n",
      "batch:1540\n",
      "batch:1550\n",
      "batch:1560\n",
      "batch:1570\n",
      "batch:1580\n",
      "batch:1590\n",
      "batch:1600\n",
      "batch:1610\n",
      "batch:1620\n",
      "batch:1630\n",
      "batch:1640\n",
      "batch:1650\n",
      "batch:1660\n",
      "batch:1670\n",
      "batch:1680\n",
      "batch:1690\n",
      "batch:1700\n",
      "batch:1710\n",
      "batch:1720\n",
      "batch:1730\n",
      "batch:1740\n",
      "batch:1750\n",
      "batch:1760\n",
      "batch:1770\n",
      "batch:1780\n",
      "batch:1790\n",
      "batch:1800\n",
      "batch:1810\n",
      "batch:1820\n",
      "batch:1830\n",
      "batch:1840\n",
      "batch:1850\n",
      "batch:1860\n",
      "batch:1870\n",
      "batch:1880\n",
      "batch:1890\n",
      "batch:1900\n",
      "batch:1910\n",
      "batch:1920\n",
      "batch:1930\n",
      "batch:1940\n",
      "batch:1950\n",
      "batch:1960\n",
      "batch:1970\n",
      "batch:1980\n",
      "batch:1990\n",
      "batch:2000\n",
      "batch:2010\n",
      "batch:2020\n",
      "batch:2030\n",
      "batch:2040\n",
      "batch:2050\n",
      "batch:2060\n",
      "batch:2070\n",
      "batch:2080\n",
      "batch:2090\n",
      "batch:2100\n",
      "batch:2110\n",
      "batch:2120\n",
      "batch:2130\n",
      "batch:2140\n",
      "batch:2150\n",
      "batch:2160\n",
      "batch:2170\n",
      "batch:2180\n",
      "batch:2190\n",
      "batch:2200\n",
      "batch:2210\n",
      "batch:2220\n",
      "batch:2230\n",
      "batch:2240\n",
      "batch:2250\n",
      "batch:2260\n",
      "batch:2270\n",
      "batch:2280\n",
      "batch:2290\n",
      "batch:2300\n",
      "batch:2310\n",
      "batch:2320\n",
      "batch:2330\n",
      "batch:2340\n",
      "batch:2350\n",
      "batch:2360\n",
      "batch:2370\n",
      "batch:2380\n",
      "batch:2390\n",
      "batch:2400\n",
      "batch:2410\n",
      "batch:2420\n",
      "batch:2430\n",
      "batch:2440\n",
      "batch:2450\n",
      "batch:2460\n",
      "batch:2470\n",
      "batch:2480\n",
      "batch:2490\n",
      "batch:2500\n",
      "batch:2510\n",
      "batch:2520\n",
      "batch:2530\n",
      "batch:2540\n",
      "batch:2550\n",
      "batch:2560\n",
      "batch:2570\n",
      "batch:2580\n",
      "batch:2590\n",
      "batch:2600\n",
      "batch:2610\n",
      "batch:2620\n",
      "batch:2630\n",
      "batch:2640\n",
      "batch:2650\n",
      "batch:2660\n",
      "batch:2670\n",
      "batch:2680\n",
      "batch:2690\n",
      "batch:2700\n",
      "batch:2710\n",
      "batch:2720\n",
      "batch:2730\n",
      "batch:2740\n",
      "batch:2750\n",
      "batch:2760\n",
      "batch:2770\n",
      "batch:2780\n",
      "batch:2790\n",
      "batch:2800\n",
      "batch:2810\n",
      "batch:2820\n",
      "batch:2830\n",
      "batch:2840\n",
      "batch:2850\n",
      "batch:2860\n",
      "batch:2870\n",
      "batch:2880\n",
      "batch:2890\n",
      "batch:2900\n",
      "batch:2910\n",
      "batch:2920\n",
      "batch:2930\n",
      "batch:2940\n",
      "batch:2950\n",
      "batch:2960\n",
      "batch:2970\n",
      "batch:2980\n",
      "Epoch 1, Loss: 6.9851\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = TransformerModel(\n",
    "    src_vocab_size=en_transformer_tokenizer.vocab_size,\n",
    "    tgt_vocab_size=de_transformer_tokenizer.vocab_size,\n",
    "    d_model=512,\n",
    "    n_head=8,\n",
    "    num_encoder_layers=6,\n",
    "    num_decoder_layers=6,\n",
    "    dim_feedforward=2048,\n",
    "    dropout=0.1,\n",
    "    max_len=64\n",
    ").to(device)\n",
    "\n",
    "# 定义优化器和损失函数\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=de_transformer_tokenizer.pad_token_id)\n",
    "\n",
    "# 训练模型\n",
    "num_epochs = 2\n",
    "for epoch in range(num_epochs):\n",
    "    train(model, sampled_loader, optimizer, criterion, device, epoch)\n",
    "    #evaluate(model, train_dataloader, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2988"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sampled_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
